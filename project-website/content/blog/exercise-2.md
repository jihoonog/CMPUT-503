---
author: "Jihoon Og"
title: "Exercise 2"
date: 
description: "Second Lab Assignment"
tags: ["bulding","programming", "ROS", "docker","odometry"]
categories: ["robot"]
ShowRelated: false
showToc: true
math: true
ShowBreadCrumbs: false
---
This is the second lab assignment of the course.

# Part 1

## ROS Subscriber and Publisher

For this exercise were tasked to implement a basic ROS subscriber and publisher that we could use later in the exercise.
The ROS framework uses a publish-subscribe architecture to send and receive data (or more specifically messages) to and from different nodes. These messages are transported using `topics`, these topics are named channels or buffers that publishers can publish messages to while subscribers can subscribe to these topics to listen to the messages. Messages are strongly typed, meaning their schema must be defined during compile time otherwise things will not work. Moreover, topics can only support a single message type, publishing multiple different data types will have all but the last topic type be overwritten by the publisher. However, you can define your own and ROS has a collection of different data types that the end user could use.

While the Duckiebot software stack is based on ROS the robot developers made a custom template repository that one could fork to implement their own ROS package(s) that can be deployed to the Duckiebots. This template repository contains a Dockerfile that is used to download the images, build the container, and run the softwares that is specific to the Duckiebots and the end-user uses it to bootstrap their project.

For making our first custom publisher and subscriber we were tasked to grab the video image from the built-in camera and publish it to a custom topic. Following the Duckietown developer guide I created a new python file that will act as my subscriber to the camera's image topic and publisher to my custom topic. I also added my new file into the launch file so that ROS will know to deploy that file as a new node. Finally I added the launch file into the the launch script that will be used to call `roslaunch` on the launch file to start my node when the container is running on the robot.

### Image Subscriber

To get the image data from the camera I first need to find the topic that contains the data.
Running `rostopic list` gave me the list of all available topics running on the duckiebot.
One stood out to me, the `/csc22935/camera_node/image/compressed` topic seemed most likely to contain the data I need based on its name. Using `rqt_image_view` and subscribing to that topic confirms my suspicion as I was able to see the images being generated by the camera.
Running `rostopic info` on the topic returned a `CompressedImage` message type used by the topic.
Running `rosmsg info` on the message type gave me the schema that represented `CompressedImage` I was only interested in the data itself and thankfully there was an attribute called `data` that was an array of unsigned bytes, the rest was just metadata.
So I created a `rospy.Subscriber` to listen on the `/csc22935/camera_node/image/compressed` topic and printing the result out gave me a bunch of random value.
Thankfully, this meant that I was getting data from the camera and the garbage mess I was reading was the JPEG encoded data of the image.
Now I just needed to publish this to my own custom topic

### Image Publisher

Republishing the image data was pretty straightforward. First I needed to create a new `CompressedImage` variable and do a deep copy of the received data as it is consumed by the subscriber. This new `CompressedImage` variable also has it header set so it knows when it was created, and published it to a custom topic.
The publisher was created using `rospy.Publisher` to a custom topic that I called `/csc22935/raw_image/compressed` and it uses the same datatype as the subscriber.

### Screenshot of the source code of the image subscriber and publisher

Below is a screenshot of the source code of my image subscriber and publisher

![image sub code](/uploads/new_subpub_node.png)

### Screenshot of the new image subscriber

Below is a screenshot of `rqt_image_view` showing the image being published to the custom topic

![image sub](/uploads/new_node_rqt_image_view.png)

## Odometry

In order to know where our robot is within its workspace based on movement alone we first need to know the robot's odometry. The motors themselves have rotor encoders that counts the number of ticks (or degrees) of rotation that each wheel have rotated. Counting the number of ticks and using a simple equation (see below) allows us to determine how far each wheel has moved laterally.

$$
\Delta X = \frac{2\cdot\pi\cdot R \cdot N_{ticks}}{N_{total}}
$$

Where: 
- {{< rawhtml >}} \(R\) {{< /rawhtml >}} is the radius
- {{< rawhtml >}} \(N_{ticks}\) {{< /rawhtml >}} is the number of ticks measured
- {{< rawhtml >}} \(N_{total}\) {{< /rawhtml >}} is the number of total ticks for a full rotation which in our case is 135.

Now that we can calculate the distance each wheel has traveled we can use transformation matrices to convert them from the robot frame to the world frame and vice versa.

1. What is the relation between your initial robot frame and world frame? How do you transform between them?

The initial world frame is 0.32, 0.32 in the x and y coordinates respectively and the theta component is
{{< rawhtml >}}
\(\dfrac{1}{2}\pi\)
{{< /rawhtml >}}.
The robot frame is 0, 0 for the x and y coordinates respectively with the theta component being the same. We can convert between the two frames using a forward and reverse kinematics equation.
To convert between the world frame to the robot frame we use this equation:

$$
\begin{bmatrix}\dot{x_I}\\\\\dot{y_I}\\\\\dot{\theta_I}\\ \end{bmatrix}  = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & 0\\\sin(\theta) & \cos(\theta) & 0\\\\ 0 & 0 & 1\\ \end{bmatrix} \begin{bmatrix}   \dot{x_R}\\\\\dot{y_R}\\\\\dot{\theta_R}\\ \end{bmatrix}
$$

2. How do you convert the location and theta at the initial robot frame to the world frame?

To convert between the robot frame to the world frame we use this equation:

$$
\begin{bmatrix}\dot{x_R}\\\\\dot{y_R}\\\\\dot{\theta_R}\\ \end{bmatrix}  = \begin{bmatrix}\cos(\theta) & \sin(\theta) & 0\\\\ -\sin(\theta) & \cos(\theta) & 0\\\\ 0 & 0 & 1\\ \end{bmatrix} \begin{bmatrix}   \dot{x_I}\\\\\dot{y_I}\\\\\dot{\theta_I}\\ \end{bmatrix}
$$

3. Can you explain why there is a difference between actual and desired location?

There could be many factors that could cause an error between the true location and the desired location. Some of these factors could include:

- Wheel slip.
- Loose tolerances within the encoders.
- Non consistent driving surface.
- No feedback mechanism to check if the motors moved the desired amount.
- Overshooting and undershooting of the desired target distance. 

4. Which topic(s) did you use to make the robot move? How did you figure out the topic that could make the motor move?

We used the `/hostname/wheels_driver_node/wheels_cmd` and published `WheelsCmdStamped` messages to move the left and right motors at a desired velocity. We figured that this topic would move the robot as we looked at the list of all available topics using `rostopic list` and using intuition guessed that this topic will move the wheels based on the descriptive topic name.

5. Which speed are you using? What happens if you increase/decrease the speed?

We used a value of `0.5` for forward movement and `0.25` for rotational movement. If we increase the speed the robot will move faster but runs the risk of overshooting the desired distance. However, decreasing the speed could prevent the robot from moving as the static friction is greater the motor's torque for the given wheel. 

6. How did you keep track of the angle rotated?

By using the following kinematic equation below:

$$\begin{bmatrix}\dot{x}_R \\\\\dot{y}_R \\\\\dot{\theta}_R \\\\ \end{bmatrix} = \begin{bmatrix}   \frac{r\dot{\varphi}_r}{2} + \frac{r\dot{\varphi}_l}{2} \\\\   0  \\\\    \frac{r\dot{\varphi}_r}{2\cdot l} - \frac{r\dot{\varphi}_l}{2\cdot l} \\\\ \end{bmatrix}$$

We can find the change of the robot's directional pose based on the left and right wheels' linear distance change.

7. Which topic(s) did you use to make the robot rotate?

We used the same topic to rotate the robot as to move the robot forward.

8. How did you estimate/track the angles your duckieBot has traveled?

Using the equation listed from the previous answer for question 6. We added all the changes of the robot's angle to the initial angle the robot started with throughout the whole execution of the robot's movement.

# Part 2

1. What is the final location of your robot as shown in your odometry reading?

The final location of the robot is: 0.2, 0.4, 90 degress for x, y, and theta respectively

1. Is it close to your robotâ€™s actual physical location in the mat world frame?

It's sometimes reasonably close. Within 30 centimeters.


## Correction

![](/uploads/diagram-20230208.svg)

$$
\vec{R}\times\vec{T} = \sin(\theta) ||\vec{R} || \cdot || \vec{T}|| u
$$

$$
\vec{R} \cdot \vec{T} = \cos(\theta) ||\vec{R} || \cdot || \vec{T}||
$$

## PID Control

$$
u(t) = K_pe(t) + K_i \int_0^te(\tau) d\tau + K_d\frac{de(t)}{dt}
$$
# References


