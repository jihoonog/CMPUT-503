[{"content":"This is the first lab assignment of the course. Graduate students needed to build their Duckiebots as well as flash the SD card and demonstrate moving the robot in a straight line and show that it can stay within a lane using the built-in Duckiebot lane-following module.\nSetting up the website Setting up the website wasn\u0026rsquo;t too difficult as it was just following the instructions on GitHub Pages with Jekyll. Installing Jekyll wasn\u0026rsquo;t too bad on my system just needed to install Ruby and install the framework using Gem. Because I have my own domain forkprocess.com I wanted to use it to point it to my course website. I use subdomains to access external web services so I created a new subdomain quackquack that will point to project\u0026rsquo;s website that is being hosted on GitHub. For deployment I used GitHub Actions to build the content of the site and deploy it automatically once something has been committed to the main repo.\nAlso, after working with Jekyll for a little bit I decided to move to Hugo as it was a framework that I was much more familiar with and can generate a blog style site faster with more diversity of themes.\nAssembling the robot Assembling the robot wasn\u0026rsquo;t too bad except for inserting the metal nuts into the chassis. I had to file down the inserts a bit so it can accept the metal nuts more easily.\nOther then that I was able to assemble the robot in about 3 hours.\nFlashing the SD card Flashing the SD card wasn\u0026rsquo;t too bad it was just long. Running this command dts init_sd_card --hostname csc22935 --type duckiebot --configuration DB21M --wifi 'DuckieNet:CMPUT412_1234!!!Rox' --no-steps verify took a little over an hour to finish. I skipped the verification step because it takes a while to do and I felt confident that the sd card flashed properly.\nNote: I had to do this multiple times because I had issues getting my bot to move/boot. To improve the process I kept the compressed image file for the Nvidia Jetson Nano used in the Duckiebot within my home directory, and copy the file over to the tmp working space that the init_sd_card program used for downloading and unpacking the image file.\nDashboard After the first boot I was able to connect to my Duckiebot\u0026rsquo;s dashboard by loading it in my web browser by accessing csc22935.local or the hostname of my Duckiebot. After the initial setup I was able to see the motor commands and camera output on the dashboard as shown below.\n Getting the robot to move and debugging pain At first the robot didn\u0026rsquo;t move using dts duckiebot keyboard_control. I followed the debugging steps, checking to see if the duckiebot-interface is up and running. Then when that didn\u0026rsquo;t resolve the problem I updated the containers which lead to other issues due to what I initially thought my docker package being incompatible with this version of dts. So I uninstalled everything and reflashed the SD card again to double check nothing when wrong during installation, same issue. Then I tried to update the Duckiebot firmware by using dts duckiebot update which lead to another issue this time an 500 server error that I originally thought was caused by docker installation as well.\nWell after many hours spent debugging the issues, doing multiple SD card reflashes and software reinstallations, I found that the problems were not from a single core issue but from multiple, seemly independent sources, here is a list to keep things brief:\n 500 Server Error: Removed my DockerHub credentials from the dts config file as that was interfering with the login process that was giving the 500 server error. Never apt-get upgrade on the Duckiebot itself as it will brink the entire system, use the dts duckiebot upgrade command instead The HUT needed to be reflashed again as it was running the wrong version but was reporting the latest version on the dashboard, thus wasn\u0026rsquo;t a clue at first. If the WiFi disconnects it\u0026rsquo;s easier to remove the WiFi adapter and plug it back in as the hotplug parameter was set during initialization therefore it will try and reconnect once the adapter is reconnected into the USB port. Don\u0026rsquo;t let your battery get too low otherwise it will lead to unexpected behaviors like improper shutdowns and WiFi disconnections. Make sure that all your micro USB cables are connected properly so that it can power on properly and prevent unnecessary SD card reflashes.  Calibrating the motors Following the guide on calibrating the motors I drove the robot in a straight line to see the drift. It it drifted left then I would decrease the trim value and if it drifted right I would increase the trim value. The trim parameter is a ratio of how much one motor should move compare to the other. If trim \u0026gt; 0 then the right motor will spin faster compared to the left and vice versa. The gain parameter is a scalar value applied to both motors on how much to spin based on the commanded signal. There is a careful balancing act between having enough gain that the robot will move when commanded at the lowest speed but lower enough that any errors aren\u0026rsquo;t amplified to be unpredictable. For both values I used binary search to find a good enough trim value such that the robot would travel straight with at most a 10 cm drift driving 2 meters. The values I settled on for trim = 0.0515 and gain = 0.85. The video below shows the straight line performance of the robot using these values.\n  Calibrating the camera Calibrating the camera was pretty easy. Following the instructions outlined in the Duckiebot documentation. I was able to calibrate both the intrinsic and extrinsic values of the robot by calling:\ndts duckiebot calibrate_intrinsics csc22935 and\ndts duckiebot calibrate_extrinsics csc22935 and follow the instructions outlined from the terminal.\nIntrinsic calibration takes into accound the small manufacturing discrepancies the lens and the sensor will have on an image. By moving the camera or bot back and forth, side to side, up and down, rotating off plane, etc I was able to get the calibration software to learn about these discrepances and make adjustments to the raw outputs so they accurately replicate reality. Extrinsic calibration tries to learn the camera\u0026rsquo;s location in 3-D space, how it\u0026rsquo;s positioned and how it\u0026rsquo;s rotated. This is important for trying to find the lane markings in Duckietown.\nLane following Running the lane following demo was fairly easy I made sure that both duckiebot-interface and car-interface was running and start the lane following container by running this command in dts:\ndts duckiebot demo --demo_name lane_following --duckiebot_name csc22935 --package_name duckietown_demos Once the container was running I opened the keyboard control for my bot and pressed the a key to start the lane following demo. The video below shows the lane following demo in action:\n  Note: I had to tune the parameters so that I can make the turns in duckietown. I had to increase the k_theta and k_d values a little bit so the robot can turn more aggressively and make the turns without going over the lines.\nColour Detection This was the most difficult part of the exercise, something I put way too much effort into when we are not going to use this code base for anything as we are going to use ROS to get the images from the camera node, but it was a good exercise nonetheless. I followed the instructions from the basic development guide from Duckietown and found that a lot of the documentation was outdated. Particularly the GStreamer pipeline code was not for the DB21 platform. I found the correct GStreamer pipeline code from the dt-duckiebot-interface repository and used that to open the camera. However, this still lead to an issue initialization the camera in OpenCV. I think the main issue is caused by missing runtime environments that GStreamer requires that isn\u0026rsquo;t being initialized by Dockerfile. I might be able to find the correct initialization procedure in the dt-duckiebot-interface repository but at this point I spent too much time trying to get this minor code to work. So I just ignored it for now.\n","permalink":"https://quackquack.forkprocess.com/blog/exercise-1/","summary":"First Lab Assignment","title":"Exercise 1"},{"content":"","permalink":"https://quackquack.forkprocess.com/contact/","summary":"How can I help you?","title":"Contact Me"},{"content":"This is the second lab assignment of the course.\nPart 1 ROS Subscriber and Publisher For this exercise were tasked to implement a basic ROS subscriber and publisher that we could use later in the exercise. The ROS framework uses a publish-subscribe architecture to send and receive data (or more specifically messages) to and from different nodes. These messages are transported using topics, these topics are named channels or buffers that publishers can publish messages to while subscribers can subscribe to these topics to listen to the messages. Messages are strongly typed, meaning their schema must be defined during compile time otherwise things will not work. Moreover, topics can only support a single message type, publishing multiple different data types will have all but the last topic type be overwritten by the publisher. However, you can define your own and ROS has a collection of different data types that the end user could use.\nWhile the Duckiebot software stack is based on ROS the robot developers made a custom template repository that one could fork to implement their own ROS package(s) that can be deployed to the Duckiebots. This template repository contains a Dockerfile that is used to download the images, build the container, and run the softwares that is specific to the Duckiebots and the end-user uses it to bootstrap their project.\nFor making our first custom publisher and subscriber we were tasked to grab the video image from the built-in camera and publish it to a custom topic. Following the Duckietown developer guide I created a new python file that will act as my subscriber to the camera\u0026rsquo;s image topic and publisher to my custom topic. I also added my new file into the launch file so that ROS will know to deploy that file as a new node. Finally I added the launch file into the the launch script that will be used to call roslaunch on the launch file to start my node when the container is running on the robot.\nImage Subscriber To get the image data from the camera I first need to find the topic that contains the data. Running rostopic list gave me the list of all available topics running on the duckiebot. One stood out to me, the /csc22935/camera_node/image/compressed topic seemed most likely to contain the data I need based on its name. Using rqt_image_view and subscribing to that topic confirms my suspicion as I was able to see the images being generated by the camera. Running rostopic info on the topic returned a CompressedImage message type used by the topic. Running rosmsg info on the message type gave me the schema that represented CompressedImage I was only interested in the data itself and thankfully there was an attribute called data that was an array of unsigned bytes, the rest was just metadata. So I created a rospy.Subscriber to listen on the /csc22935/camera_node/image/compressed topic and printing the result out gave me a bunch of random value. Thankfully, this meant that I was getting data from the camera and the garbage mess I was reading was the JPEG encoded data of the image. Now I just needed to publish this to my own custom topic\nImage Publisher Republishing the image data was pretty straightforward. First I needed to create a new CompressedImage variable and do a deep copy of the received data as it is consumed by the subscriber. This new CompressedImage variable also has it header set so it knows when it was created, and published it to a custom topic. The publisher was created using rospy.Publisher to a custom topic that I called /csc22935/raw_image/compressed and it uses the same datatype as the subscriber.\nScreenshot of the source code of the image subscriber and publisher Below is a screenshot of the source code of my image subscriber and publisher\n Screenshot of the new image subscriber Below is a screenshot of rqt_image_view showing the image being published to the custom topic\n Odometry In order to know where our robot is within its workspace based on movement alone we first need to know the robot\u0026rsquo;s odometry. The motors themselves have rotor encoders that counts the number of ticks (or degrees) of rotation that each wheel have rotated. Counting the number of ticks and using a simple equation (see below) allows us to determine how far each wheel has moved laterally.\n$$ \\Delta X = \\frac{2\\cdot\\pi\\cdot R \\cdot N_{ticks}}{N_{total}} $$\nWhere:\n  \\(R\\) is the radius  \\(N_{ticks}\\) is the number of ticks measured  \\(N_{total}\\) is the number of total ticks for a full rotation which in our case is 135.  Now that we can calculate the distance each wheel has traveled we can use transformation matrices to convert them from the robot frame to the world frame and vice versa.\n What is the relation between your initial robot frame and world frame? How do you transform between them?  The initial world frame is 0.32, 0.32 in the x and y coordinates respectively and the theta component is \\(\\dfrac{1}{2}\\pi\\) . The robot frame is 0, 0 for the x and y coordinates respectively with the theta component being the same. We can convert between the two frames using a forward and reverse kinematics equation. To convert between the world frame to the robot frame we use this equation:\n$$ \\begin{bmatrix}\\dot{x_I}\\\\\\dot{y_I}\\\\\\dot{\\theta_I}\\ \\end{bmatrix} = \\begin{bmatrix}\\cos(\\theta) \u0026amp; -\\sin(\\theta) \u0026amp; 0\\\\sin(\\theta) \u0026amp; \\cos(\\theta) \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\ \\end{bmatrix} \\begin{bmatrix} \\dot{x_R}\\\\\\dot{y_R}\\\\\\dot{\\theta_R}\\ \\end{bmatrix} $$\nHow do you convert the location and theta at the initial robot frame to the world frame?  To convert between the robot frame to the world frame we use this equation:\n$$ \\begin{bmatrix}\\dot{x_R}\\\\\\dot{y_R}\\\\\\dot{\\theta_R}\\ \\end{bmatrix} = \\begin{bmatrix}\\cos(\\theta) \u0026amp; \\sin(\\theta) \u0026amp; 0\\\\ -\\sin(\\theta) \u0026amp; \\cos(\\theta) \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\ \\end{bmatrix} \\begin{bmatrix} \\dot{x_I}\\\\\\dot{y_I}\\\\\\dot{\\theta_I}\\ \\end{bmatrix} $$\nCan you explain why there is a difference between actual and desired location?  There could be many factors that could cause an error between the true location and the desired location. Some of these factors could include:\n Wheel slip. Loose tolerances within the encoders. Non consistent driving surface. No feedback mechanism to check if the motors moved the desired amount. Overshooting and undershooting of the desired target distance.  Which topic(s) did you use to make the robot move? How did you figure out the topic that could make the motor move?  We used the /hostname/wheels_driver_node/wheels_cmd and published WheelsCmdStamped messages to move the left and right motors at a desired velocity. We figured that this topic would move the robot as we looked at the list of all available topics using rostopic list and using intuition guessed that this topic will move the wheels based on the descriptive topic name.\nWhich speed are you using? What happens if you increase/decrease the speed?  We used a value of 0.5 for forward movement and 0.25 for rotational movement. If we increase the speed the robot will move faster but runs the risk of overshooting the desired distance. However, decreasing the speed could prevent the robot from moving as the static friction is greater the motor\u0026rsquo;s torque for the given wheel.\nHow did you keep track of the angle rotated?  By using the following kinematic equation below:\n$$\\begin{bmatrix}\\dot{x}_R \\\\\\dot{y}_R \\\\\\dot{\\theta}_R \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{r\\dot{\\varphi}_r}{2} + \\frac{r\\dot{\\varphi}_l}{2} \\\\ 0 \\\\ \\frac{r\\dot{\\varphi}_r}{2\\cdot l} - \\frac{r\\dot{\\varphi}_l}{2\\cdot l} \\\\ \\end{bmatrix}$$\nWe can find the change of the robot\u0026rsquo;s directional pose based on the left and right wheels' linear distance change.\nWhich topic(s) did you use to make the robot rotate?  We used the same topic to rotate the robot as to move the robot forward.\nHow did you estimate/track the angles your duckieBot has traveled?  Using the equation listed from the previous answer for question 6. We added all the changes of the robot\u0026rsquo;s angle to the initial angle the robot started with throughout the whole execution of the robot\u0026rsquo;s movement.\nPart 2  What is the final location of your robot as shown in your odometry reading?  The final location of the robot is: 0.2, 0.4, 90 degress for x, y, and theta respectively\n Is it close to your robotâ€™s actual physical location in the mat world frame?  It\u0026rsquo;s sometimes reasonably close. Within 30 centimeters.\nCorrection  PID Control $$ u(t) = K_pe(t) + K_i \\int_0^te(\\tau) dt $$\nReferences ","permalink":"https://quackquack.forkprocess.com/blog/exercise-2/","summary":"Second Lab Assignment","title":"Exercise 2"}]