[{"content":"Grad Final Project This is my final grad project for CMPUT-503, it\u0026rsquo;s an extension of lab exercise 5 where we used a machine learning model to predict single digit numbers that was trained on MNIST. For this project I decided to run the model directly on the duckiebot as our previous lab exercise ran the model on a laptop that was getting data from the duckiebot.\nMotivation In lab exercise 5 we were running a ML model on our laptop as even the smallest ML framework library were too big to fit onto the duckiebot. However, this communication overhead lead to some performance issues as network congestion lead to poor inference performance and system lag as data packets where being dropped and being forced to re-sent. Running the model locally on the duckiebot should avoid this overhead as all communication is being done locally on the robot.\nModel Training Pipeline Installing TensorFlow Lite or any other lightweight ML framework is difficult as the library package is too big to run on the duckiebot itself. One could implement their own feed-forward and backpropagation module for the the duckiebot but that would be too time consuming and difficult. Instead, I trained a Multi-Layer Perceptron (MLP) model using Pytorch on a machine with GPU-compute and transferred the weights to the duckiebot to run in inference mode. This way I only need to implement the inference portion for the duckiebot which for a simple MLP model is trivial to do.\nModel The model is a relatively simple 4-layer, fully connected model that has 784 inputs from a flatten 28 by 28 gray-scale image with 10 outputs, each representing the prediction strength of a digit. Between each layer there is a ReLU activation function, and a dropout layer set to 10% to help with over-fitting.\nThe diagram below shows the model architecture:\n Loss function and Optimizer Cross-Entropy loss was used as the loss function as this was a classification task on predicting the correct digit or label. The Pytorch Adam optimizer was used as it showed good convergence performance for training and validation.\nDataset The MNIST training dataset was used to train the model, with a split ratio of 90:10 for training and validation respectively. The MNIST test dataset was used to verfiy that the model will work however, the actual test number used for the experiment was completely different.\nBelow is a image of the test number used for the experiment:\n Training The model was trained for 10 epoch and had a test loss of 0.066 and a test accuracy of 97.99%. The weights that performed the best were exported to the duckiebot for the experiment.\nVideo demo The video below shows our model that is running on the duckiebot in action.\n   The green bounding box shows the detected region of the post-it note that contains a number. The cyan number above the bounding box shows the predicted model generated by the model. The yellow time on the right of the predicted number, shows the total time it took the system to predict a number.  Repo Link Grad project repository link\n","permalink":"https://quackquack.forkprocess.com/blog/grad-project/","summary":"CMPUT-503 Grad Project","title":"Grad Final Project"},{"content":"Team Members  Jihoon Og Qianxi Li  Final Project In this final project, we needed to combine all the knowledge we learned from the previous 5 exercises together and finish 3 separate tasks using our Duckiebot.\nStage 1 We have a Duckietown like the one below, the first stage is to start at the start position, the Duckiebot should move autonomously and following the commands from the Apriltags in Duckietown. Each tag is assigned with one kind of traffic signal, like stop, turn, and go straight. The desired route for the Duckiebot is to follow the purple route on the diagram below.\nThe diagram below shows one of the route that the Duckiebot needs to take for stage 1. The other route is taking the outside lane of the loop.\n Stage 2 After stage 1 is finished, it will go straight until it see a pedestrian walk or a duck walk in this case and make a short stop, if there are ducks waddling through the duck walk it will stop until it has been cleared. Otherwise, it will continue ahead. It then makes a turn, and the Duckiebot should stop, then move around the broken Duckiebot on the road and avoid run into it. It then needs to successfully handle a second duck walk following the same rules as the first one.\nThe diagram below shows what the Duckiebot needs to do to complete stage 2.\n Stage 3 The last stage is to park the Duckiebot into one of the 4 stalls. Which stall to park into is determined during the start of the demo.\nThe diagram below shows the 4 different stalls and the path to park in them.\n Implementation strategies Here we briefly describe our implementation for the final project.\nDetection To make the Duckiebot see, we utilized multiple different techniques for detecting different kinds of objects. For this project, things that need to be detected include:\n White and yellow lanes on the road Red stop lines Apriltags Obstacle detection (like the broken Duckiebot, yellow rubber duck, blue lane for pedestrian crossing)  Broken Car As the Duckiebot move toward the broken Duckiebot on the road, it is able to see the circular grid pattern on the back of it. In this case, we use the template code from exercise 4, which is about detecting the Duckiebot from the camera. For distance calculation we used the laser range finder because it is more accurate in determining the distance.\nApril Tag This was prett straightforward, the Apriltag detector has been used in many tasks in the past several exercises. One special things here is that, we needed the pose for each detection so that we can keep the closest detection to the Duckiebot, otherwise it will give up multiple detection and multiple conflicting commands.\nYellow duck and blue lane for pedestrian crossing Instead of using machine learning techniques to do the duck detection, we just used a simple color masks to mask out everything that is not yellow. The high-level idea is: first, we detect the stop sign Apriltag, then we detect the blue pedestrian crossing using the color mask, since the blue is unique around the stop sign Apriltag we just need to know whether there\u0026rsquo;s something blue in the image and on the road. Next, for the duck detection, we just need to know whether there\u0026rsquo;s something yellow in the image and on the blue lane, so color masking was used to find the ducks. However, because the yellow colour of the duck is similar to the yellow color of the median, we added an area threshold to only detect yellow that is big enough to be rubber duck sized and not a yellow lane marking.\nParking For parking we store all the parking stalls in a list where the stall number corresponds to the index representing the Apriltag ID for the stall. Parking is initiated based on the successful detection of the Apriltag that at the entrance of the parking lot. After stopping at the entrance of the parking lot it then drives into the parking lot making a right or left turn based on the stall number then using the Apriltag pose in robot frame, drive into the stall using the lateral distance (y) and rotational offset on the z axis to make correctional movements using PID control drive into the stall. This works okay if the robot is not too far off center both lateral and rotationally. As we didn\u0026rsquo;t have much time to tune the PID control for driving into the parking stall correctly. Moreover, if the camera lost track of the Apriltag from either getting too close and going out of view or too far away then it will constantly generate the same control output as we didn\u0026rsquo;t implement a timeout mechanism.\nPID For PID control for lane following we used the same settings and mechanism as in exercise 3 as it worked very well. We tried to implement PID control for velocity and for parking but due to time limitations we couldn\u0026rsquo;t implement them well in time.\nVideo demo Below shows a video demo of our bot doing all three stages.\n  Repo Link Final project repository link\nReferences  https://docs.photonvision.org/en/latest/docs/getting-started/pipeline-tuning/apriltag-tuning.html https://docs.duckietown.com/daffy/ https://github.com/duckietown/lib-dt-apriltags  Work Division  Jihoon: PID control, use detection results and make the robot make the right decision (turn, go straight, stop, avoid duck and broken car) Qianxi: All the detection things. Detect Apriltag, red and blue lanes and rubber ducks. Almas: Parking  ","permalink":"https://quackquack.forkprocess.com/blog/final-project/","summary":"CMPUT-503 Final Project","title":"Final Project"},{"content":"Team Members  Jihoon Og Qianxi Li  Exercise 5: Machine Learning for Robotics In this exercise we learned some of the basics of machine learning, how to train and validate a deep learning model, how layers like dropout works, how data augmentation techniques like random crop, image normalization, and random rotation work. After getting a trained model, we deployed it in a Duckiebot container to handle a single-digit number recognition task. The goal is to have the Duckiebot do lane following and exploring the Duckietown environment, and once a digit is detected from the image input, report the figure, the Apriltag it is associated with, the location of the Apriltag and memorize this detection. All nodes should stop once all 10 digits are found.\nDeliverable 1 The figure below shows the result of doing one iteration of backpropagation. The forward pass lets us calculate a direct error of the predicted label compared to the actual label. Using backpropagation lets us propagate the gradient back to each layer of the computation graph so that they can update with respect to the changes correspondingly. Because the prediction error is smaller in this iteration compared to the previous iteration shows that the model is learning.\n Deliverable 2 Questions and Answers  What data augmentation is used in training? Please delete the data augmentation and rerun the code to compare.  The image has been randomly rotated between positive and negative 5 degrees, in addition a random crop is added to the training image where a 2 pixel padding is applied to the image before taking a random 28x28 square crop essentially shifting the number within the image. Finally, the data is then normalized in a PyTorch tensor. With the transformations the test accuracy was 97.91%. Without the transformation the test accuracy was 97.88%.\nWhat is the batch size in the code? Please change the batch size to 16 and 1024 and explain the variation in results.  The default batch size is 64. With batch size 16 the test accuracy was 97.85%, but training took much longer (twice as long) as with using a batch size of 64. With batch size 1024 the test accuracy was 97.45% and training still took a lot longer but still faster than training with a batch size of 16. The reason for the difference in training time could be caused by the overhead of transferring the mini-batches from the CPU\u0026rsquo;s memory to the GPU\u0026rsquo;s memory. A larger batch size will take longer to process but have less of them to process. While a smaller batch size will take less time but more of them to process. The overhead of transferring the images to the GPU\u0026rsquo;s memory is multiplied by the number of batches, and this overhead scales with the number of images being transferred. Therefore, there is a happy medium where the training throughput is maximum while the total overhead of data being transferred is minimized.\nWhat activation function is used in the hidden layer? Please replace it with the linear activation function and see how the training output differs. Show your results before and after changing the activation function in your written report.  The default activation function is ReLU or Rectified Linear Unit. Using a linear activation function caused the test accuracy to dropped to 83.53%. In contrast, using the ReLU activation function lead to a 98% accuracy for the test set.\n What is the optimization algorithm in the code? Explain the role of optimization algorithm in training process  The default optimization algorithm used is Adam. The optimization algorithm is used to update the parameters of the model with respect to the loss calculated during training.\nAdd dropout in the training and explain how the dropout layer helps in training.  I added dropout layers after the 2 hidden layers after the non-linear activation function with a dropout probability of 0.1 or 10%, and it produced a test accuracy of 97.58% on the test set. Dropout is useful when your model is overfitting on the training dataset. Because dropout randomly sets some nodes to 0, it effectively creates a new model where other nodes can learn from the training data.\nDeliverable 3 In this section we trained a machine learning model to learn single-digit numbers from the MNIST dataset to recognize the single-digit numbers attached to the AprilTags in the Duckietown environment.\nHere is the model architecture that we used.\n Since the convolutional neural network is specialized in processing image information, we used it to extract high-level features and use those features in a fully connected layer so that it can generate a vector of length 10. This will tell us the prediction probability for the inputted image to be one of the labels. We then can use the argmax function to get the estimated label with the highest probability.\nWe also used data augmentation techniques like random crop, normalization, and random rotation to make our model more robust to the input with different orientations and lighting conditions. In addition, we trained the model for 5 epochs and the final validation accuracy is around 98.2%.\nThere is room for improvement since 100 channels may not be necessary for our task and environment. Our next iteration if we had time would consider using dropout layers and tune the number of channels to be as if not more robust but less complex than the current model.\nThe video below shows our Duckiebot driving around the Duckietown environment detecting and identifying the numbers and stopping once all 10 digits have been correctly identified.\n  As you can see in the video, the Duckiebot starts from a random location in Duckietown, and start to do lane following automatically. It will also detect the Apriltags and the numbers with blue backgrounds. Once both are detected, the Apriltag\u0026rsquo;s location and the number detection\u0026rsquo;s result will be printed out to the terminal, the detection history will also be memorized. The bounding box of the detected number will be visible through RViz. The Duckiebot randomly makes a turn or goes straight at intersections to explore the whole Duckietown, once all 10 figures are found, all nodes terminate. Overall it finished the task, but there were some problems with the system design which we will discuss them below.\nFor the detection part, since we know the figures are black and the background blue, we can just use OpenCV to mask and create bounding boxes to shrink down our detection region to a small rectangle, instead of the whole image. The image is then converted to black and white, resize to 28 by 28 and feed into the CNN to make predictions. The actual prediction is the most frequent prediction for the (number, AprilTag) pair in the detection history, we did this because sometimes the detection region for a figure can be inaccurate from a relative far distance, as it moves closer, the bounding box will be more accurate, and we don\u0026rsquo;t want the detection result to be unstable.\nWe can see from the video that sometimes the detection cannot be done quickly. Several reasons for this:\n The duckiebot didn\u0026rsquo;t stay close enough and observe the figure long enough before it made its turn or passed the figure. Sometimes the robot drove past the AprilTag, but the figure in the image can be close to the edge of the image and undistorted. Network delays: In the last 1-2 minutes of the video, the network started to get delayed, if the model cannot get the current video frame, then it cannot make the right prediction. (See the image below, this black and white image was fed into the model throughout the whole duration where figure 9 appeared, and the model cannot make the right prediction). We need a better way to prevent lane following from rushing out of the Duckietown environment. The current design can make sure the robot can move randomly left, right, or straight at the intersection but these options are not always available. We ran all the nodes on the laptop, but a better solution would be to run the figure detection on the laptop and the rest on the robot to improve performance.   How well did your implemented strategy work? Was it reliable? In what situations did it perform poorly? Our implemented strategy could finish the task, it\u0026rsquo;s okay to use but not perfect and not 100% reliable. Some situations could make it perform poorly:\n Network issue (for sure), high delay in communication. Having someone wearing a shirt with the same blue color as the figure background has. We can give more restricts to the detection region to solve this problem, but yes, our current design cannot handle this :( ). The bounding box detector can also be senstive to the light condition in the room. Driving too fast can make it miss the target.  Repo Link Exercise 5 repository link\nReferences This is a list of references that I used to do this exercise.\n Backprop: https://hmkcode.com/ai/backpropagation-step-by-step/ “Multilayer Perceptron.ipynb” uploaded on eClass  ","permalink":"https://quackquack.forkprocess.com/blog/exercise-5/","summary":"Fifth Lab Assignment","title":"Exercise 5"},{"content":"This is the fourth lab assignment of the course.\nTeam Members  Jihoon Og Qianxi Li  Exercise 4 Don\u0026rsquo;t Crash! Trailing Behaviour For this exercise we were tasked on implementing a following behaviour on our Duckiebot where it will follow a leader bot if it detects one around the Duckietown environment. This involves combining different components from previous exercises such as lane following, Apriltag detection, and custom LED emitter patterns. With new components like vehicle detection and distance calculation. All of these components are fed into a single node that handles the robot and lane following behaviour.\nVideo The video below shows the Duckiebot with the yellow duck following a leader bot around the Duckietown environment. It stops at intersections and signals its intention to other robots to its rear. We didn\u0026rsquo;t use the front LED as it could affect the computer vision system for detecting the stop line and leader bot. Note, there are some instances where the robot stops for a prolong period of time. This was caused by network congestion that prevented the robot from getting new data as the ROS master was running locally on a laptop, not the robot for faster processing.\n  Brief Implementation Strategy Each subsection below contains a brief paragraph describing our implementation strategy for maintaining a safe driving distance and avoiding collisions\nLane following For lane following we took our implementation from exercise 3 and incorporated it into this exercise. It worked well for the Duckietown environment and we decided to reused it for this exercise with some minor parameter changes to the PID controllers for lane following.\nLeader Robot Detection, Distance Calculation, and Collision Avoidance For robot detection and distance calculation we decided to use the duckiebot_detection_node and duckiebot_distance_node provided to use for this exercise as it provided a reasonably accurate detection and distance calculation based on the vehicle tag on the back of the Duckiebot. To utilize this information we have two callback functions, one for detecting if a robot exist within its camera\u0026rsquo;s field-of-view, another is to store the distance from the robot. In the get_control_action function which is responsible for moving the robot in a controlled manner a check is done to see if the distance to the leader bot is below a set threshold. If so it will stop the robot until the leader bot move beyond the set threshold.\nIntersection Detection and Handling For intersection detection and handling we based our code from the stop_line_filter_node from dt-core. At a high-level it takes all red line segments generated from the lane detector node from the Duckiebot lane following pipeline and convert them into lane frame (or its frame of reference from the robot). It then takes the average of all the x values from the converted line segments and determine the distance from the red stop line. If the distance is within a set threshold then the robot stops at the red stop line following the rules of the road and either follows the leader bot or drives autonomously using via lane following if it loses track of the leader bot. The robot also signal its intention when its handling intersections, with flashing turn signals and red brake lights when it\u0026rsquo;s stopped at intersections.\nVehicle Tracking at Intersections For tracking the lead vehicle at intersections we compute the average x values from the 21 dots behind the Duckiebot as seen from the camera. Depending if the averaged x value is on the left, center, or right side of the image we can make an educated guess on where the lead robot will go. To improve robustness in the system we take a history and take the most common prediction at an intersection. If no lead bot is detected then a None value is added to the prediction causing the robot to drive autonomously (lane follow) around Duckietown.\nBrief Dissions on Results Questions  How well did your implemented strategy work?  Our implemented strategy worked well for lane following, vehicle detection, stop line detection, and going straight at an intersection. This is because most of our implementation is based on Duckietown\u0026rsquo;s implementation that we just built upon.\n Was it reliable?  It was mostly reliable when it was lane following and following behind the leader bot without making any turns at an intersection i.e., going straight. Any complicated behaviours like right turns at intersections where it can lose track behind an april tag or non-precise movements from the leader bot lead to less than ideal behaviour.\nIn what situations did it perform poorly?  If the leader bot makes a right turn at an intersection then the follower bot might lose track as the rear vehicle tag is hidden behind the Apriltags causing it to lose track and revert back to lane following which if possible defaults to driving straight at an intersection.\nRepo Link Exercise 4 repository link\nReferences This is a list of references that I used to do this exercise.\n Lane Controller Node: https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/src/lane_controller_node.py Lane Controller: https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/include/lane_controller/controller.py PID Controller: https://en.wikipedia.org/wiki/PID_controller PID controller code: https://github.com/jellevos/simple-ros-pid/blob/master/simple_pid/PID.py Stop line filter node code: https://github.com/duckietown/dt-core/blob/daffy/packages/stop_line_filter/src/stop_line_filter_node.py  ","permalink":"https://quackquack.forkprocess.com/blog/exercise-4/","summary":"Fourth Lab Assignment","title":"Exercise 4"},{"content":"This is the third lab assignment of the course.\nTeam Members  Jihoon Og Qianxi Li  Part 1 - Computer Vision Part 1 was primarily done by my lab partner Qianxi Li, the report is mostly from his website here\nFor this exercise, we were required to interact with the transformation between different frames in the Duckietown environment. We first needed to recognize the different AprilTags using the built-in computer vision library. For section 1 of exercise 3, we basically needed to undo the distortion caused by the camera, then use the AprilTags detector library to get the tag\u0026rsquo;s id, location, rotation pose, and corner locations within the camera frame. We then needed to label the edge of the tag and give a type to it. Depending on the type of the AprilTag, we needed to change the color of the LED emitter based on the table below:\n Red: Stop Sign Blue: T-Intersection Green: U of A Tag White: No Detections  In certain situations the camera may capture multiple tags within it\u0026rsquo;s field of view. However, we only want to detect the closest one. So we only keep the one with the smallest z-value (closest to our camera) and the clamp the maximum z-value to be accepted to 0.5 meters. In addition, the margin of our detection should be larger than 10 to get classified as an AprilTag with a strong confidence.\nBecause the lens of the camera distort our image, the lines within the image are not straight. Therefore, when people deal with 3-D tasks from photo image input, an undistortion stage is required.\nQuestions  What does the april tag library return to you for determining its position?  For each tag it detects, it returns the x, y coordinates for the tag\u0026rsquo;s center and its corners. It uses the image coordinate system meaning the origin starts in the top left corner and increases as you go right and down. It can also estimate the pose of the tag with respect to the camera in (x,y,z).\nWhich directions do the X, Y, Z values of your detection increase / decrease?  The z-axis points from the camera\u0026rsquo;s center out from the camera\u0026rsquo;s lens. The x-axis is to the right in the image taken by the camera. The y-axis goes down from the top frame of the camera. So moving away from the camera will increase the z value, moving to the right of the camera will increase the x value, and moving downward of the camera will crease the y value.\nWhat frame orientation does the april tag use?  The tag\u0026rsquo;s coordinate frame is centered at the center of the tag, with the x-axis to the right, y-axis pointing down, and the z-axis pointing into the tag.\nhttps://github.com/AprilRobotics/apriltag/wiki/AprilTag-User-Guide\nWhy are detections from far away prone to error?  Tags that are further away will make it harder for the camera to distinguish the smaller \u0026ldquo;pixels\u0026rdquo; that identifies the AprilTags. Moreover, the focal length is set for a particular distance from the camera and objects that are farther away from the focal length will be out-of-focus thus making it harder for the detection algorithm to find and identify the tag.\nWhy may you want to limit the rate of detections?  Detecting the AprilTags too frequently will add unnecessary overhead to the system adding additional latency that could negatively affect other systems that are critical.\nVideo The video below shows how the Duckiebot detects and labels the AprilTag using a bounding box and a text label. A tag with a type of intersection is detected by the camera.\n Part 2 - Lane Following This part was primarily done by me.\nImplementation In order to ease development of the lane following functionality we\u0026rsquo;ve decided to reuse the lane following pipeline from the dt-core module. Below is a diagram from the Duckietown documentation showing an overview of the lane following pipeline.\n For this lab exercise we\u0026rsquo;ve implemented our own lane controller node that take in lane pose generated by the lane filter node from the Duckiebot\u0026rsquo;s built-in lane following pipeline. The main difference between their implementation and mine is that We use all three PID parameters (Proportional, Integral, and Derivative) for both the lateral and angular error while they use only proportional and integral terms for the lateral and angular error. The main rational is to further improve performance by using a D term to decrease overshoot as well settling time.\nQuestions  What is the error for your PID controller?  We used two errors, one is the lateral error, and the other is the angular error from the centerline. While we could just use the lateral error for correction, we\u0026rsquo;ve decided to use bot the lateral and angular error as they are provided by the lane filter node. Moreover, having the additional angular error will help with the lane following as the Duckiebot could be in the center of a lane but at an off angle where moving forward will cause it to drift out of the lane.\nIf your proportional controller did not work well alone, what could have caused this?  Using proportional control led to decent on-center performance on a straight lane tile, however it led to poor on-center performance during a turn. This is because the proportional term only handles errors happening at present time. On a straight lane if the robot was already aligned with the center of the lane very little correction is needed so long as the trim is set correctly. However, during a turn the center of the lane is constantly moving thus requiring the Duckiebot to make constant corrections. If the proportional term is too small then it can\u0026rsquo;t make a large enough correction to stay within the lane. However, if the term is too large then it will over-correct and create oscillation during the straights. Moreover, a happy medium doesn\u0026rsquo;t really exist that works well for both turns and straights. Therefore, we needed to give more feedback.\nDoes the D term help your controller logic? Why or why not?  Adding a D term helped a lot with the overshoot from having too high of a P term. Because the D term tries to look at future trends in error it provides a dampening component if the P and I components leads to an overshoot.\n(Optional) Why or why not was the I term useful for your robot?  The I term was useful at the turning tiles as there was a constant change in direction as well as the location of the center line with respect to the Duckiebot\u0026rsquo;s lateral axis. As the error grows over time the I component will add more and more corrective effort in order to bring the Duckiebot back to the center of the lane.\nVideo Demo The video below shows the robot performing some basic lane following driving on the right lane:\n  The video below shows the robot performing some basic lane following driving on the left lane:\n  Part 3 - Localization Using Sensor Fusion Part 3 was primarily done by my lab partner Qianxi Li, the report is mostly from his website [here](https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3\nThe top-level goal for this part is to utilize:\n The known locations of all AprilTags in the world frame (from part 1) Use the node we wrote in part 1 to detect the presence of April tags in the image to get a better lane following in a world shown below.   As we are using the lane following system from part 2, if we detect an AprilTag the camera, we then can figure out where the robot is in the world frame and update its pose to the true location, otherwise, we continue to use odometry which may be inaccurate.\nThe image above is the Duckietown environment our Duckiebots are interacting with, in the next several sections, our Duckiebots will travel along the blue lane, and we will examine the odometry and how the frames of robot interact with each other.\nSection 3.1-3.2 In these two sections, the two primary things we needed to get familiar with were:\n Using Rviz to visualize different frames and transformations in different forms Given some fixed landmarks in the world, compare the path the robot traveled measured in world frame with the measurements produced by odometry.  We need to use the method static_transform_publisher of TF2 to broadcast the locations and orientations of 10 different Apriltags in the world frame. Rviz can then be used to visualize all of their locations in the world frame.\nQuestions  Where did your odometry seem to drift the most? Why would that be?  It drifted to the right more frequently. One reason could be that it\u0026rsquo;s hard to make a perfect 90-degree turn by only looking at the camera image, usually when it makes the turn, you feel that it’s a little too over, so you adjust the yaw to the opposite angle a little, which confuse the odometry. The lane we are following has 4 right turns, so it’s easy to get errors at each turn and drift to the right.\nDid adding the landmarks make it easier to understand where and when the odometry drifted?  Yes, adding the landmarks is very easy to see the current location of our Duckiebot and get to know where and when it is drifted.\nSection 3.3 In this section, we need to obtain a transform tree graph, which is a tree structure with nodes being different frames and edges being transformations.\nTo generate the graph, we did the following procedure:\n Go to dashboard Open portainer Open a console of the ROS container Install ros-noetic-tf2-tools Run rosrun tf2_tools view_frames.py to get the PDF file Copy it to /data and download it.  The transform tree graph looks like this, you can see the root is vehicle_name/footprint\n Section 3.4 For this section, we can visualize all the frames in the Duckiebot using RViz, and figure out which joint is responsible for rotating the wheels.\nQuestion  What\u0026rsquo;s the type of joint that moves when we move the wheels?  The joint is continuous. Since we need to rotate the wheel to move it and a continuous joint can rotate around the axis and has no upper and lower limits.\nSection 3.5 In this section, we want to see both the frames on a Duckiebot, and it should also change the location and orientation in the world frame if we move it. To do this, we let the parent frame be the odometry frame and the child frame be the /footprint frame, since the odometry frame is the child to the world frame, the root is now the world frame. The image below shows the current transform graph:\n The reason why no frame is moving is that we are now using the baselink frame (/footprint) of the Duckiebot, whenever you move the robot, the frame will also be there and the relative location of different frames on a Duckiebot will be the same.\nQuestions  What should the translation and rotation be from the odometry child to robot parent frame? In what situation would you have to use something different?  The translation and rotation should all be zero. We should consider changing the values when the robot root frame is not the same as the odometry frame.\nAfter creating this link generate a new transform tree graph. What is the new root/parent frame for your environment?  The new root is /world, the world frame.\nCan a frame have two parents? What is your reasoning for this?  No, a frame cannot have two parents. The transform tree graph means, for every two nodes in the same tree, it is possible to have a transformation between them, but if you have two parents of the same frame, you cannot have a transformation that transforms one to another. And TF expects a tree structure and cannot deal with the case that has multiple parents.\nCan an environment have more than one parent/root frame?  Yes, an environment can have more than one parent frame, just like before 3.5, we have a parent world frame with child AprilTag static frames and the odometry frame. And another parent footprint frame with a couple of other frames on the robot. And you cannot have one frame in a tree that transforms into another frame in another tree.\nSection 3.6 The video below shows how the robot moves in the world frame, where are the estimated location of the AprilTags, and the ground truth locations visualized by RViz. The delay is high since at the time we record this video, there were many people working in the lab causing high network contention. The estimated location of the AprilTags is obtained in the following way:\n We have the rotation and location information in the camera frame from the raw image Then we transformed that from the camera frame to the estimated location relative to the camera frame. You can also see all the frames on the robot are also moving as the robot moves, that\u0026rsquo;s because in section 3.5 we attached the odometry frame with the robot root frame.   Questions  How far off are your detections from the static ground truth?  If an AprilTag is detected in the image, the estimated AprilTag\u0026rsquo;s frame will show up in RViz immediately. One issue with our design is that after one detection disappeared from the image, the last broadcasted frame will still be there and it will also rotate and move relative to the camera until we have a new April tag detected.\nThe error between the ground truth and the estimation is within 20 centimeters, if there\u0026rsquo;s a tag detected in the current image.\nWhat are two factors that could cause this error?    The resolution of the image can be low, since we can control the resolution of the image we pass to the detector, the higher it is, the more computation we will do, the more accurate location of the tag we can obtain (corner, edge\u0026hellip;)\n  The light and the angle at that the camera observes the tag also matter. The dimmer the room is, the smaller the angle is, the harder for the camera to estimate its location and other information.\n  Section 3.7 This section combines the previous sections were we use camera input as the sensor fusion to help localize our robot in the Duckietown.\nThe robot travels along the lane shown in the image below, when it detects an Apriltag, it will estimate its location based on the tag\u0026rsquo;s information. When no Apriltag is available, it will just use odometry.\n From the video below, you can see in the RViz window, all the April tag frames and all the frames on the Duckiebot are visualized, along with the greyscaled, undistorted camera input that shows where the robot it and what it sees\n The network was slow at the time we record the video so sometimes the delay can be high. We were using keyboard control to make it move. As you can see in the video, each time when it detects a tag, it will transport it to a more accurate location in the world, the robot is located at the place where a lot of different frames overlapping together since we are also displaying other frames on the robot. Due to the wifi\u0026rsquo;s problem, it sometimes takes a while for the robot to transport it to the right location and publish the current camera image.\nWith the help of the sensor fusion, now the robot travels pretty well, there\u0026rsquo;s only a minor difference between the end location and the start location, and that is also because the wifi is slow so we need to decrease the frequency of the detection and use more odometry.\nQuestions  Is this a perfect system?  Of course not. Although we are using geometry to calculate where the robot actually is in the world frame based on sensor fusion, the detection is not in real-time so there might be a delay for updating location, and the geometry can also bring larger error if the robot is observing the tag in a large angle (look from the side).\nWhat are the causes for some of the errors?   Wifi. There were a lot of people at the lab when we record the videos, sometimes I failed to build my images and connect to my robot. Because of this, I decreased the detection frequency to 1 per second and use more odometry, which prunes to errors sometimes. The estimated April tag location based on the detection result. There\u0026rsquo;s always going to be an error here since you cannot estimate the tag location perfectly.  What other approaches could you use to improve localization?   It would be nice to have more than one camera, just like when you use 2 eyes to observe the world, you can always get a better sense of where things are located, then you can get a better sense of where you are located. Use lane following instead of keyboard control. Control the robot manually usually make the odometry more confusing since you can accidentally make a turn too over and need to adjust back to the right direction, I did this multiple times and it always makes the odometry lost completely. Optimize our detector to make it faster.  What I learned from this exercise For this exercise I learned how to implement PID control for following the lanes within Duckietown. Moreover, I learned how to reuse the existing lane following pipeline to implement my own lane following node using PID control.\nRepo Link Exercise 3 repository link\nReferences This is a list of references that I used to do this exercise.\n Lane Controller Node: https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/src/lane_controller_node.py Lane Controller: https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/include/lane_controller/controller.py PID Controller: https://en.wikipedia.org/wiki/PID_controller PID controller code: https://github.com/jellevos/simple-ros-pid/blob/master/simple_pid/PID.py Qianxi Li\u0026rsquo;s website: https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3 URDF Joints: http://wiki.ros.org/urdf/XML/joint URDF parameters for the Duckiebot: https://github.com/duckietown/dt-duckiebot-interface/blob/56a299aa5739e7f03a6b96d3b8dac3a8beca532c/packages/duckiebot_interface/urdf/duckiebot.urdf.xacro TF2: http://wiki.ros.org/tf2/Tutorials/Introduction%20to%20tf2 TF2 Static Broadcaster: http://wiki.ros.org/tf2/Tutorials/Writing%20a%20tf2%20static%20broadcaster%20%28Python%29 TF transformations: http://wiki.ros.org/tf/Overview/Transformations Sensor Fusion: https://docs.duckietown.org/daffy/duckietown-classical-robotics/out/exercise_sensor_fusion.html#fig:rviz-final-tf-tree AprilTag with Python: https://pyimagesearch.com/2020/11/02/apriltag-with-python/  ","permalink":"https://quackquack.forkprocess.com/blog/exercise-3/","summary":"Third Lab Assignment","title":"Exercise 3"},{"content":"This is the second lab assignment of the course.\nPart 1 ROS Subscriber and Publisher For this exercise were tasked to implement a basic ROS subscriber and publisher that we could use later in the exercise. The ROS framework uses a publish-subscribe architecture to send and receive data (or more specifically messages) to and from different nodes. These messages are transported using topics, these topics are named channels or buffers that publishers can publish messages to while subscribers can subscribe to these topics to listen to the messages. Messages are strongly typed, meaning their schema must be defined during compile time otherwise things will not work. Moreover, topics can only support a single message type, publishing multiple different data types will have all but the last topic type be overwritten by the publisher. However, you can define your own and ROS has a collection of different data types that the end user could use.\nWhile the Duckiebot software stack is based on ROS the robot developers made a custom template repository that one could fork to implement their own ROS package(s) that can be deployed to the Duckiebots. This template repository contains a Dockerfile that is used to download the images, build the container, and run the software programs that is specific to the Duckiebot and the end-user uses it to bootstrap their project.\nFor making our first custom publisher and subscriber we were tasked to grab the video image from the built-in camera and publish it to a custom topic. Following the Duckietown developer guide I created a new python file that will act as my subscriber to the camera\u0026rsquo;s image topic and publisher to my custom topic. I also added my new file into the launch file so that ROS will know to deploy that file as a new node. Finally, I added the launch file into the launch script that will be used to call roslaunch on the launch file to start my node when the container is running on the robot.\nImage Subscriber To get the image data from the camera I first need to find the topic that contains the data. Running rostopic list gave me the list of all available topics running on the duckiebot. One stood out to me, the /csc22935/camera_node/image/compressed topic seemed most likely to contain the data I need based on its name. Using rqt_image_view and subscribing to that topic confirms my suspicion as I was able to see the images being generated by the camera. Running rostopic info on the topic returned a CompressedImage message type used by the topic. Running rosmsg info on the message type gave me the schema that represented CompressedImage I was only interested in the data itself and thankfully there was an attribute called data that was an array of unsigned bytes, the rest was just metadata. So I created a rospy.Subscriber to listen on the /csc22935/camera_node/image/compressed topic and printing the result out gave me a bunch of random value. Thankfully, this meant that I was getting data from the camera and the garbage mess I was reading was the JPEG encoded data of the image. Now I just needed to publish this to my own custom topic\nImage Publisher Republishing the image data was pretty straightforward. First I needed to create a new CompressedImage variable and do a deep copy of the received data as it is consumed by the subscriber. This new CompressedImage variable also has it header set, so it knows when it was created, and published it to a custom topic. The publisher was created using rospy.Publisher to a custom topic that I called /csc22935/raw_image/compressed, and it uses the same datatype as the subscriber.\nScreenshot of the source code of the image subscriber and publisher Below is a screenshot of the source code of my image subscriber and publisher\n Screenshot of the new image subscriber Below is a screenshot of rqt_image_view showing the image being published to the custom topic\n Odometry In order to know where our robot is within its workspace based on movement alone we first need to know the robot\u0026rsquo;s odometry. The motors themselves have rotor encoders that counts the number of ticks (or degrees) of rotation that each wheel have rotated. Counting the number of ticks and using a simple equation (see below) allows us to determine how far each wheel has moved laterally.\n$$ \\Delta X = \\frac{2\\cdot\\pi\\cdot R \\cdot N_{ticks}}{N_{total}} $$\nWhere:\n  \\(R\\) is the radius  \\(N_{ticks}\\) is the number of ticks measured  \\(N_{total}\\) is the number of total ticks for a full rotation which in our case is 135.  Now that we can calculate the distance each wheel has traveled we can use transformation matrices to convert them from the robot frame to the world frame and vice versa.\n What is the relation between your initial robot frame and world frame? How do you transform between them?  The initial world frame is 0.32, 0.32 in the x and y coordinates respectively and the theta component is \\(\\dfrac{\\pi}{2}\\) . The robot frame is 0, 0, 0 for the x, y, and theta component respectively. We can convert between the two frames using a forward and reverse kinematics equation. To convert between the world frame to the robot frame we use this equation:\n$$ \\begin{bmatrix}\\dot{x_I}\\\\\\dot{y_I}\\\\\\dot{\\theta_I}\\ \\end{bmatrix} = \\begin{bmatrix}\\cos(\\theta) \u0026amp; -\\sin(\\theta) \u0026amp; 0\\\\sin(\\theta) \u0026amp; \\cos(\\theta) \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\ \\end{bmatrix} \\begin{bmatrix} \\dot{x_R}\\\\\\dot{y_R}\\\\\\dot{\\theta_R}\\ \\end{bmatrix} + \\begin{bmatrix} 0.32 \\\\ 0.32 \\\\ \\frac{\\pi}{2}\\end{bmatrix} $$\nThe last term is the offset term that is used to account for the different origins between the robot frame and the world frame.\nHow do you convert the location and theta at the initial robot frame to the world frame?  To convert between the robot frame to the world frame we use this equation:\n$$ \\begin{bmatrix}\\dot{x_R}\\\\\\dot{y_R}\\\\\\dot{\\theta_R}\\ \\end{bmatrix} = \\begin{bmatrix}\\cos(\\theta) \u0026amp; \\sin(\\theta) \u0026amp; 0\\\\ -\\sin(\\theta) \u0026amp; \\cos(\\theta) \u0026amp; 0\\\\ 0 \u0026amp; 0 \u0026amp; 1\\ \\end{bmatrix} \\begin{bmatrix} \\dot{x_I}\\\\\\dot{y_I}\\\\\\dot{\\theta_I}\\ \\end{bmatrix} - \\begin{bmatrix} 0.32 \\\\ 0.32 \\\\ \\frac{\\pi}{2}\\end{bmatrix} $$\nLikewise in the previous answer, the last term is the offset term that is used to account for the different origins between the robot frame and the world frame.\nCan you explain why there is a difference between actual and desired location?  There could be many factors that could cause an error between the true location and the desired location. Some of these factors could include:\n Wheel slip. Loose tolerances within the encoders. Non-consistent driving surface. No feedback mechanism to check if the motors moved the desired amount. Overshooting and undershooting of the desired target distance.  Which topic(s) did you use to make the robot move? How did you figure out the topic that could make the motor move?  We used the /hostname/wheels_driver_node/wheels_cmd and published WheelsCmdStamped messages to move the left and right motors at a desired velocity. We figured that this topic would move the robot as we looked at the list of all available topics using rostopic list and using intuition guessed that this topic will move the wheels based on the descriptive topic name.\nWhich speed are you using? What happens if you increase/decrease the speed?  We used a value of 0.6 for forward movement and 0.6 for rotational movement. If we increase the speed the robot will move faster but runs the risk of overshooting the desired distance. However, decreasing the speed could prevent the robot from moving as the static friction is greater the motor\u0026rsquo;s torque.\nHow did you keep track of the angle rotated?  By using the following kinematic equation below:\n$$\\begin{bmatrix}\\dot{x}_R \\\\\\dot{y}_R \\\\\\dot{\\theta}_R \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{r\\dot{\\varphi}_r}{2} + \\frac{r\\dot{\\varphi}_l}{2} \\\\ 0 \\\\ \\frac{r\\dot{\\varphi}_r}{2\\cdot l} - \\frac{r\\dot{\\varphi}_l}{2\\cdot l} \\\\ \\end{bmatrix}$$\nWe can find the change of the robot\u0026rsquo;s directional pose based on the left and right wheels' linear distance change.\nWhich topic(s) did you use to make the robot rotate?  We used the same topic to rotate the robot as to move the robot forward.\nHow did you estimate/track the angles your duckieBot has traveled?  Using the equation listed from the previous answer for question 6. We added all the changes of the robot\u0026rsquo;s angle to the initial angle the robot started with throughout the whole execution of the robot\u0026rsquo;s movement.\nWhile we can implement these equations into the motor control node that is used to estimate the robot\u0026rsquo;s pose and correct for drift. We would much rather use the Duckietown\u0026rsquo;s implementation [1] for the Duckiebot as it is more likely to be correct and robust compared to our implementation. For example, in their implementation they use AppromixateTime to synchronize the timestamp between the two encoder messages so that the message from one encoder is close to the timestamp of the other. The code reference is linked below.\nPart 2 In this part we are tasked to create a multi-state program where our robot will move about in the lab\u0026rsquo;s Duckietown environment following a pre-programed route and setting the LED light pattern to indicate the robot\u0026rsquo;s current state.\nArchitecture Below is the ROS computation graph for implementing this exercise. It is comprised with two nodes:\n The state_control_node is responsible for maintaining the current state of the robot as well as its state transitions and setting the LED patterns. It also give commands to the motor_control_node to move the robot to a specified location in world frame. The motor_control_node is responsible for moving the robot to a specific location based on commands received from state_control_node. It handles all odometry calculations, dead reckoning, error corrections, and motor control for the robot. Once the current command is successfully completed an acknowledgement is sent back to the state_control_node indicating that the robot finished the current task.   The State Control Node The state control node as stated before is responsible for maintaining the current state, state transition, and setting of the LEDs for the robot. In our implementation states are executed in a sequential fashion. Once a command in published to the motor_control_node the state_control_node is blocked until it receives a confirmation that the command was successfully completed. This confirmation comes from the motor_control_node. Once the confirmation message is received it can then move onto publishing the next command to the motor_control_node. The commands themselves are fairly simple, due to the simple tasks that our robot needed to do. The commands are a formatted string that is published to the motor_control_node using a String type message. An example command could be \u0026quot;forward:2.3\u0026quot; meaning move forward 2.3 meters from the current world frame. Another could be \u0026quot;right:80\u0026quot; meaning rotate right (clockwise) 80 degrees. Parsing is done at the motor_control_node.\nLED light pattern For setting the light patterns for different stages of the task, we first run this command to launch the led_emitter_node:\ndts duckiebot demo --demo_name led_emitter_node --duckiebot_name $BOT --package_name led_emitter --image duckietown/dt-core:daffy-arm64v8 We then use the service \u0026lt;VEHICLE_NAME\u0026gt;/led_emitter_node/set_custom_pattern to set the different LED patterns to their corresponding state. Since we need to call this service multiple times, we keep the connection persistent.\nThe colour pattern for each state is defined below:\n Red Blue Green Purple  The Motor Control Node The motor control node as stated before is responsible for all movement command executions, odometry calculations, dead reckoning, error corrections, and motor control for the robot. The motor_control_node is more of a listener to the state_control_node, it doesn\u0026rsquo;t do anything until it receives a command from the controller that is the state_control_node. Once it receives a command it then executes that command to the best of it\u0026rsquo;s ability. There are three functions that implement the movements required for the lab exercise. One moves the robot forward by a specified amount in meters one rotates the robot by a specified amount in degrees, and one moves the robot in an arcing motion that is kinda hacky.\nCommands received are appended to a list where it is used to update and correct the robot\u0026rsquo;s pose. It also provides a useful debugging tool to see any errors in the robot\u0026rsquo;s odometry. For forward movement there is a vector that connects the robot\u0026rsquo;s pre-movement position to its target position. The robot then follows that vector to the target position. For rotational movement the robot will rotate in place until the robot\u0026rsquo;s theta odometry is close to the target\u0026rsquo;s direction. The rotation in-place is done by having one motor spin in one direction and the other motor spin in the other direction at the same speed so that there isn\u0026rsquo;t any translational movement during rotation. For the arc movement one of the motor is spinning faster than the other so that it can create both a translational and rotational movement.\nCorrections Due to manufacturing defects, loose tolerances, the unpredictable nature of reality, and the fact that we can\u0026rsquo;t assume a spherical duck. There will be some error or drift between the target pose and the actual pose. In this exercise we were not required to implement close-loop control but I found it easier to implement some kind of control loop feedback that made it easier to do all the required tasks without driving off Duckietown. There are many process variables (PV) that we could have use to have our robot drive in a straight line:\n The difference in distances each wheel has traveled The drift between the target track and the robot\u0026rsquo;s position to that track  But the one that works best for us, was the angle between the robot vector and the target vector. The diagram below shows a visual representation of the two vectors.\n   \\(\\vec{R}\\) is the robot vector which describes where the robot is heading. This is derived from the robot\u0026rsquo;s odometry.  \\(\\vec{T}\\) is the target vector which describes the heading to the target position from the robot\u0026rsquo;s position.  Minimizing the angle between the robot vector and the target vector while driving forward should get our robot to the desired location. To get the angle between the robot vector and the target vector we can use the dot product divided by the product magnitude of the two vectors to get the cosine value where taking the inverse gives us the angle.\n$$ \\frac{\\vec{R} \\cdot \\vec{T}}{||\\vec{R} || \\cdot || \\vec{T}||} = \\cos(\\theta) \\rightarrow \\arccos(\\cos(\\theta)) = \\theta $$\nHowever, this does not gives us the direction on where the target vector lies in relation to the robot vector. For that we need the cross product of the two vectors to find the sin value that will gives the direction. If the value is less than 0 then the target is right of the robot, if greater than 0 then the target is left of the robot.\n$$ \\frac{\\vec{R}\\times\\vec{T}}{||\\vec{R} || \\cdot || \\vec{T}|| \\cdot u} = \\sin(\\theta) $$\nNow that we have the magnitude and direction of the error we can add this into our close-loop feedback system which in this case is PID control.\nPID Control A PID (proportional-integral-derivative) controller is a commonly used feedback control-loop mechanism that gives corrections to a process or a plant such that its output or process value matches the desired set-point [2]. It uses three tunable parameters that takes into account the present error, past errors, and an estimate of future errors to provide a correction value such that it minimizes over-corrective oscillation and unnecessary delay. The equation below is the overall control function:\n$$ u(t) = K_pe(t) + K_i \\int_0^te(\\tau) d\\tau + K_d\\frac{de(t)}{dt} $$\nWhere:\n  \\(K_p\\) is the proportional gain, a tuning parameter,  \\(K_i\\) is the integral gain, a tuning parameter,  \\(K_d\\) is the derivative gain, a tuning parameter,  \\(e(t)\\) is the error between the set-point or target point and process variable at time \\(t\\) ,  \\(t\\) is the time,  \\(\\tau\\) is the variable of integration (takes on values from time 0 to the present \\(t\\) ).  Now tuning these parameters could be a course all in itself and I had a limited amount of time so I guessed and checked by tuning each parameter separately and settled on the parameters listed below:\n  \\(K_p = 0.4\\)   \\(K_i = 0.075\\)   \\(K_d = 0.0\\)   We used someone elses PID controller [3] for implementing PID control.\n What is the final location of your robot as shown in your odometry reading?  The final location of the robot is: 0.39, 0.53, ~86.7 degrees for x, y, and theta respectively\n Is it close to your robot’s actual physical location in the mat world frame?  Using Euclidean distance the difference was 22.14 centimeters.\nVideo The video below shows the robot performing some basic pre-planned maneuvers in Duckietown:\n  The video below shows the robot\u0026rsquo;s odometry over time while performing the same basic pre-planned maneuvers from the video above:\n  ROS Bag Bag file\nRepo Link Exercise 2 repository link\nReferences This is a list of references that I used to do this exercise.\n Deadreckoning: https://github.com/duckietown/dt-core/blob/daffy/packages/deadreckoning/src/deadreckoning_node.py PID Controller: https://en.wikipedia.org/wiki/PID_controller PID controller code: https://github.com/jellevos/simple-ros-pid/blob/master/simple_pid/PID.py  ","permalink":"https://quackquack.forkprocess.com/blog/exercise-2/","summary":"Second Lab Assignment","title":"Exercise 2"},{"content":"This is the first lab assignment of the course. Graduate students needed to build their Duckiebots as well as flash the SD card and demonstrate moving the robot in a straight line and show that it can stay within a lane using the built-in Duckiebot lane-following module.\nSetting up the website Setting up the website wasn\u0026rsquo;t too difficult as it was just following the instructions on GitHub Pages with Jekyll. Installing Jekyll wasn\u0026rsquo;t too bad on my system just needed to install Ruby and install the framework using Gem. Because I have my own domain forkprocess.com I wanted to use it to point it to my course website. I use subdomains to access external web services so I created a new subdomain quackquack that will point to project\u0026rsquo;s website that is being hosted on GitHub. For deployment I used GitHub Actions to build the content of the site and deploy it automatically once something has been committed to the main repo.\nAlso, after working with Jekyll for a little bit I decided to move to Hugo as it was a framework that I was much more familiar with and can generate a blog style site faster with more diversity of themes.\nAssembling the robot Assembling the robot wasn\u0026rsquo;t too bad except for inserting the metal nuts into the chassis. I had to file down the inserts a bit so it can accept the metal nuts more easily.\nOther then that I was able to assemble the robot in about 3 hours.\nFlashing the SD card Flashing the SD card wasn\u0026rsquo;t too bad it was just long. Running this command dts init_sd_card --hostname csc22935 --type duckiebot --configuration DB21M --wifi 'DuckieNet:CMPUT412_1234!!!Rox' --no-steps verify took a little over an hour to finish. I skipped the verification step because it takes a while to do and I felt confident that the sd card flashed properly.\nNote: I had to do this multiple times because I had issues getting my bot to move/boot. To improve the process I kept the compressed image file for the Nvidia Jetson Nano used in the Duckiebot within my home directory, and copy the file over to the tmp working space that the init_sd_card program used for downloading and unpacking the image file.\nDashboard After the first boot I was able to connect to my Duckiebot\u0026rsquo;s dashboard by loading it in my web browser by accessing csc22935.local or the hostname of my Duckiebot. After the initial setup I was able to see the motor commands and camera output on the dashboard as shown below.\n Getting the robot to move and debugging pain At first the robot didn\u0026rsquo;t move using dts duckiebot keyboard_control. I followed the debugging steps, checking to see if the duckiebot-interface is up and running. Then when that didn\u0026rsquo;t resolve the problem I updated the containers which lead to other issues due to what I initially thought my docker package being incompatible with this version of dts. So I uninstalled everything and reflashed the SD card again to double check nothing when wrong during installation, same issue. Then I tried to update the Duckiebot firmware by using dts duckiebot update which lead to another issue this time an 500 server error that I originally thought was caused by docker installation as well.\nWell after many hours spent debugging the issues, doing multiple SD card reflashes and software reinstallations, I found that the problems were not from a single core issue but from multiple, seemly independent sources, here is a list to keep things brief:\n 500 Server Error: Removed my DockerHub credentials from the dts config file as that was interfering with the login process that was giving the 500 server error. Never apt-get upgrade on the Duckiebot itself as it will brink the entire system, use the dts duckiebot upgrade command instead The HUT needed to be reflashed again as it was running the wrong version but was reporting the latest version on the dashboard, thus wasn\u0026rsquo;t a clue at first. If the WiFi disconnects it\u0026rsquo;s easier to remove the WiFi adapter and plug it back in as the hotplug parameter was set during initialization therefore it will try and reconnect once the adapter is reconnected into the USB port. Don\u0026rsquo;t let your battery get too low otherwise it will lead to unexpected behaviors like improper shutdowns and WiFi disconnections. Make sure that all your micro USB cables are connected properly so that it can power on properly and prevent unnecessary SD card reflashes.  Calibrating the motors Following the guide on calibrating the motors I drove the robot in a straight line to see the drift. It it drifted left then I would decrease the trim value and if it drifted right I would increase the trim value. The trim parameter is a ratio of how much one motor should move compare to the other. If trim \u0026gt; 0 then the right motor will spin faster compared to the left and vice versa. The gain parameter is a scalar value applied to both motors on how much to spin based on the commanded signal. There is a careful balancing act between having enough gain that the robot will move when commanded at the lowest speed but lower enough that any errors aren\u0026rsquo;t amplified to be unpredictable. For both values I used binary search to find a good enough trim value such that the robot would travel straight with at most a 10 cm drift driving 2 meters. The values I settled on for trim = 0.0515 and gain = 0.85. The video below shows the straight line performance of the robot using these values.\n  Calibrating the camera Calibrating the camera was pretty easy. Following the instructions outlined in the Duckiebot documentation. I was able to calibrate both the intrinsic and extrinsic values of the robot by calling:\ndts duckiebot calibrate_intrinsics csc22935 and\ndts duckiebot calibrate_extrinsics csc22935 and follow the instructions outlined from the terminal.\nIntrinsic calibration takes into accound the small manufacturing discrepancies the lens and the sensor will have on an image. By moving the camera or bot back and forth, side to side, up and down, rotating off plane, etc I was able to get the calibration software to learn about these discrepances and make adjustments to the raw outputs so they accurately replicate reality. Extrinsic calibration tries to learn the camera\u0026rsquo;s location in 3-D space, how it\u0026rsquo;s positioned and how it\u0026rsquo;s rotated. This is important for trying to find the lane markings in Duckietown.\nLane following Running the lane following demo was fairly easy I made sure that both duckiebot-interface and car-interface was running and start the lane following container by running this command in dts:\ndts duckiebot demo --demo_name lane_following --duckiebot_name csc22935 --package_name duckietown_demos Once the container was running I opened the keyboard control for my bot and pressed the a key to start the lane following demo. The video below shows the lane following demo in action:\n  Note: I had to tune the parameters so that I can make the turns in duckietown. I had to increase the k_theta and k_d values a little bit so the robot can turn more aggressively and make the turns without going over the lines.\nColour Detection This was the most difficult part of the exercise, something I put way too much effort into when we are not going to use this code base for anything as we are going to use ROS to get the images from the camera node, but it was a good exercise nonetheless. I followed the instructions from the basic development guide from Duckietown and found that a lot of the documentation was outdated. Particularly the GStreamer pipeline code was not for the DB21 platform. I found the correct GStreamer pipeline code from the dt-duckiebot-interface repository and used that to open the camera. However, this still lead to an issue initialization the camera in OpenCV. I think the main issue is caused by missing runtime environments that GStreamer requires that isn\u0026rsquo;t being initialized by Dockerfile. I might be able to find the correct initialization procedure in the dt-duckiebot-interface repository but at this point I spent too much time trying to get this minor code to work. So I just ignored it for now.\n","permalink":"https://quackquack.forkprocess.com/blog/exercise-1/","summary":"First Lab Assignment","title":"Exercise 1"},{"content":"","permalink":"https://quackquack.forkprocess.com/contact/","summary":"How can I help you?","title":"Contact Me"}]