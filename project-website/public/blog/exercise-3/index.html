<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Exercise 3 | Piko</title><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Third Lab Assignment"><meta name=author content="Jihoon Og"><link rel=canonical href=https://quackquack.forkprocess.com/blog/exercise-3/><link rel=stylesheet href=https://quackquack.forkprocess.com/sass/main.min.css><link rel=stylesheet href=https://quackquack.forkprocess.com/sass/nav.min.css><link rel=stylesheet href=https://quackquack.forkprocess.com/plugins/css/pico.min.css><script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://quackquack.forkprocess.com/uploads/zap.svg><link rel=icon type=image/png sizes=16x16 href=https://quackquack.forkprocess.com/uploads/zap.svg><link rel=icon type=image/png sizes=32x32 href=https://quackquack.forkprocess.com/uploads/zap.svg><link rel=apple-touch-icon href=https://quackquack.forkprocess.com/uploads/zap.svg><link rel=mask-icon href=https://quackquack.forkprocess.com/uploads/zap.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.92.2"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://quackquack.forkprocess.com/uploads/og.webp"><meta name=twitter:title content="Exercise 3"><meta name=twitter:description content="Third Lab Assignment"><meta property="og:title" content="Exercise 3"><meta property="og:description" content="Third Lab Assignment"><meta property="og:type" content="article"><meta property="og:url" content="https://quackquack.forkprocess.com/blog/exercise-3/"><meta property="og:image" content="https://quackquack.forkprocess.com/uploads/og.webp"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-03-03T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-03T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://quackquack.forkprocess.com/uploads/og.webp"><meta name=twitter:title content="Exercise 3"><meta name=twitter:description content="Third Lab Assignment"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://quackquack.forkprocess.com/blog/"},{"@type":"ListItem","position":2,"name":"Exercise 3","item":"https://quackquack.forkprocess.com/blog/exercise-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Exercise 3","name":"Exercise 3","description":"Third Lab Assignment","keywords":["lane following","programming","ROS","pid","sensor fusion"],"articleBody":"This is the third lab assignment of the course.\nTeam Members  Jihoon Og Qianxi Li  Part 1 - Computer Vision Part 1 was primarily done by my lab partner Qianxi Li, the report is mostly from his website here\nFor this exercise, we were required to interact with the transformation between different frames in the Duckietown environment. We first needed to recognize the different AprilTags using the built-in computer vision library. For section 1 of exercise 3, we basically needed to undo the distortion caused by the camera, then use the AprilTags detector library to get the tag’s id, location, rotation pose, and corner locations within the camera frame. We then needed to label the edge of the tag and give a type to it. Depending on the type of the AprilTag, we needed to change the color of the LED emitter based on the table below:\n Red: Stop Sign Blue: T-Intersection Green: U of A Tag White: No Detections  In certain situations the camera may capture multiple tags within it’s field of view. However, we only want to detect the closest one. So we only keep the one with the smallest z-value (closest to our camera) and the clamp the maximum z-value to be accepted to 0.5 meters. In addition, the margin of our detection should be larger than 10 to get classified as an AprilTag with a strong confidence.\nBecause the lens of the camera distort our image, the lines within the image are not straight. Therefore, when people deal with 3-D tasks from photo image input, an undistortion stage is required.\nQuestions  What does the april tag library return to you for determining its position?  For each tag it detects, it returns the x, y coordinates for the tag’s center and its corners. It uses the image coordinate system meaning the origin starts in the top left corner and increases as you go right and down. It can also estimate the pose of the tag with respect to the camera in (x,y,z).\nWhich directions do the X, Y, Z values of your detection increase / decrease?  The z-axis points from the camera’s center out from the camera’s lens. The x-axis is to the right in the image taken by the camera. The y-axis goes down from the top frame of the camera. So moving away from the camera will increase the z value, moving to the right of the camera will increase the x value, and moving downward of the camera will crease the y value.\nWhat frame orientation does the april tag use?  The tag’s coordinate frame is centered at the center of the tag, with the x-axis to the right, y-axis pointing down, and the z-axis pointing into the tag.\nhttps://github.com/AprilRobotics/apriltag/wiki/AprilTag-User-Guide\nWhy are detections from far away prone to error?  Tags that are further away will make it harder for the camera to distinguish the smaller “pixels” that identifies the AprilTags. Moreover, the focal length is set for a particular distance from the camera and objects that are farther away from the focal length will be out-of-focus thus making it harder for the detection algorithm to find and identify the tag.\nWhy may you want to limit the rate of detections?  Detecting the AprilTags too frequently will add unnecessary overhead to the system adding additional latency that could negatively affect other systems that are critical.\nVideo The video below shows how the Duckiebot detects and labels the AprilTag using a bounding box and a text label. A tag with a type of intersection is detected by the camera.\n Part 2 - Lane Following This part was primarily done by me.\nImplementation In order to ease development of the lane following functionality we’ve decided to reuse the lane following pipeline from the dt-core module. Below is a diagram from the Duckietown documentation showing an overview of the lane following pipeline.\n For this lab exercise we’ve implemented our own lane controller node that take in lane pose generated by the lane filter node from the Duckiebot’s built-in lane following pipeline. The main difference between their implementation and mine is that We use all three PID parameters (Proportional, Integral, and Derivative) for both the lateral and angular error while they use only proportional and integral terms for the lateral and angular error. The main rational is to further improve performance by using a D term to decrease overshoot as well settling time.\nQuestions  What is the error for your PID controller?  We used two errors, one is the lateral error, and the other is the angular error from the centerline. While we could just use the lateral error for correction, we’ve decided to use bot the lateral and angular error as they are provided by the lane filter node. Moreover, having the additional angular error will help with the lane following as the Duckiebot could be in the center of a lane but at an off angle where moving forward will cause it to drift out of the lane.\nIf your proportional controller did not work well alone, what could have caused this?  Using proportional control led to decent on-center performance on a straight lane tile, however it led to poor on-center performance during a turn. This is because the proportional term only handles errors happening at present time. On a straight lane if the robot was already aligned with the center of the lane very little correction is needed so long as the trim is set correctly. However, during a turn the center of the lane is constantly moving thus requiring the Duckiebot to make constant corrections. If the proportional term is too small then it can’t make a large enough correction to stay within the lane. However, if the term is too large then it will over-correct and create oscillation during the straights. Moreover, a happy medium doesn’t really exist that works well for both turns and straights. Therefore, we needed to give more feedback.\nDoes the D term help your controller logic? Why or why not?  Adding a D term helped a lot with the overshoot from having too high of a P term. Because the D term tries to look at future trends in error it provides a dampening component if the P and I components leads to an overshoot.\n(Optional) Why or why not was the I term useful for your robot?  The I term was useful at the turning tiles as there was a constant change in direction as well as the location of the center line with respect to the Duckiebot’s lateral axis. As the error grows over time the I component will add more and more corrective effort in order to bring the Duckiebot back to the center of the lane.\nVideo Demo The video below shows the robot performing some basic lane following driving on the right lane:\n  The video below shows the robot performing some basic lane following driving on the left lane:\n  Part 3 - Localization Using Sensor Fusion Part 3 was primarily done by my lab partner Qianxi Li, the report is mostly from his website [here](https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3\nThe top-level goal for this part is to utilize:\n The known locations of all AprilTags in the world frame (from part 1) Use the node we wrote in part 1 to detect the presence of April tags in the image to get a better lane following in a world shown below.   As we are using the lane following system from part 2, if we detect an AprilTag the camera, we then can figure out where the robot is in the world frame and update its pose to the true location, otherwise, we continue to use odometry which may be inaccurate.\nThe image above is the Duckietown environment our Duckiebots are interacting with, in the next several sections, our Duckiebots will travel along the blue lane, and we will examine the odometry and how the frames of robot interact with each other.\nSection 3.1-3.2 In these two sections, the two primary things we needed to get familiar with were:\n Using Rviz to visualize different frames and transformations in different forms Given some fixed landmarks in the world, compare the path the robot traveled measured in world frame with the measurements produced by odometry.  We need to use the method static_transform_publisher of TF2 to broadcast the locations and orientations of 10 different Apriltags in the world frame. Rviz can then be used to visualize all of their locations in the world frame.\nQuestions  Where did your odometry seem to drift the most? Why would that be?  It drifted to the right more frequently. One reason could be that it’s hard to make a perfect 90-degree turn by only looking at the camera image, usually when it makes the turn, you feel that it’s a little too over, so you adjust the yaw to the opposite angle a little, which confuse the odometry. The lane we are following has 4 right turns, so it’s easy to get errors at each turn and drift to the right.\nDid adding the landmarks make it easier to understand where and when the odometry drifted?  Yes, adding the landmarks is very easy to see the current location of our Duckiebot and get to know where and when it is drifted.\nSection 3.3 In this section, we need to obtain a transform tree graph, which is a tree structure with nodes being different frames and edges being transformations.\nTo generate the graph, we did the following procedure:\n Go to dashboard Open portainer Open a console of the ROS container Install ros-noetic-tf2-tools Run rosrun tf2_tools view_frames.py to get the PDF file Copy it to /data and download it.  The transform tree graph looks like this, you can see the root is vehicle_name/footprint\n Section 3.4 For this section, we can visualize all the frames in the Duckiebot using RViz, and figure out which joint is responsible for rotating the wheels.\nQuestion  What’s the type of joint that moves when we move the wheels?  The joint is continuous. Since we need to rotate the wheel to move it and a continuous joint can rotate around the axis and has no upper and lower limits.\nSection 3.5 In this section, we want to see both the frames on a Duckiebot, and it should also change the location and orientation in the world frame if we move it. To do this, we let the parent frame be the odometry frame and the child frame be the /footprint frame, since the odometry frame is the child to the world frame, the root is now the world frame. The image below shows the current transform graph:\n The reason why no frame is moving is that we are now using the baselink frame (/footprint) of the Duckiebot, whenever you move the robot, the frame will also be there and the relative location of different frames on a Duckiebot will be the same.\nQuestions  What should the translation and rotation be from the odometry child to robot parent frame? In what situation would you have to use something different?  The translation and rotation should all be zero. We should consider changing the values when the robot root frame is not the same as the odometry frame.\nAfter creating this link generate a new transform tree graph. What is the new root/parent frame for your environment?  The new root is /world, the world frame.\nCan a frame have two parents? What is your reasoning for this?  No, a frame cannot have two parents. The transform tree graph means, for every two nodes in the same tree, it is possible to have a transformation between them, but if you have two parents of the same frame, you cannot have a transformation that transforms one to another. And TF expects a tree structure and cannot deal with the case that has multiple parents.\nCan an environment have more than one parent/root frame?  Yes, an environment can have more than one parent frame, just like before 3.5, we have a parent world frame with child AprilTag static frames and the odometry frame. And another parent footprint frame with a couple of other frames on the robot. And you cannot have one frame in a tree that transforms into another frame in another tree.\nSection 3.6 The video below shows how the robot moves in the world frame, where are the estimated location of the AprilTags, and the ground truth locations visualized by RViz. The delay is high since at the time we record this video, there were many people working in the lab causing high network contention. The estimated location of the AprilTags is obtained in the following way:\n We have the rotation and location information in the camera frame from the raw image Then we transformed that from the camera frame to the estimated location relative to the camera frame. You can also see all the frames on the robot are also moving as the robot moves, that’s because in section 3.5 we attached the odometry frame with the robot root frame.   Questions  How far off are your detections from the static ground truth?  If an AprilTag is detected in the image, the estimated AprilTag’s frame will show up in RViz immediately. One issue with our design is that after one detection disappeared from the image, the last broadcasted frame will still be there and it will also rotate and move relative to the camera until we have a new April tag detected.\nThe error between the ground truth and the estimation is within 20 centimeters, if there’s a tag detected in the current image.\nWhat are two factors that could cause this error?    The resolution of the image can be low, since we can control the resolution of the image we pass to the detector, the higher it is, the more computation we will do, the more accurate location of the tag we can obtain (corner, edge…)\n  The light and the angle at that the camera observes the tag also matter. The dimmer the room is, the smaller the angle is, the harder for the camera to estimate its location and other information.\n  3.7  Is this a perfect system? What are the causes for some of the errors? What other appraoches could you use to improve localization?  Repo Link Exercise 3 repository link (TODO)\nReferences This is a list of references that I used to do this exercise.\n Lane Controller Node: https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/src/lane_controller_node.py Lane Controller: https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/include/lane_controller/controller.py PID Controller: https://en.wikipedia.org/wiki/PID_controller PID controller code: https://github.com/jellevos/simple-ros-pid/blob/master/simple_pid/PID.py Qianxi Li’s website: https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3 URDF Joints: http://wiki.ros.org/urdf/XML/joint URDF parameters for the Duckiebot: https://github.com/duckietown/dt-duckiebot-interface/blob/56a299aa5739e7f03a6b96d3b8dac3a8beca532c/packages/duckiebot_interface/urdf/duckiebot.urdf.xacro TF2: http://wiki.ros.org/tf2/Tutorials/Introduction%20to%20tf2 TF2 Static Broadcaster: http://wiki.ros.org/tf2/Tutorials/Writing%20a%20tf2%20static%20broadcaster%20%28Python%29 TF transformations: http://wiki.ros.org/tf/Overview/Transformations Sensor Fusion: https://docs.duckietown.org/daffy/duckietown-classical-robotics/out/exercise_sensor_fusion.html#fig:rviz-final-tf-tree AprilTag with Python: https://pyimagesearch.com/2020/11/02/apriltag-with-python/  ","wordCount":"2443","inLanguage":"en","datePublished":"2023-03-03T00:00:00Z","dateModified":"2023-03-03T00:00:00Z","author":{"@type":"Person","name":"Jihoon Og"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://quackquack.forkprocess.com/blog/exercise-3/"},"publisher":{"@type":"Organization","name":"Piko","logo":{"@type":"ImageObject","url":"https://quackquack.forkprocess.com/uploads/zap.svg"}}}</script></head><body><nav class=desktop><ul><li><a class=logo href=https://quackquack.forkprocess.com/ accesskey=h title="Welcome to my CMPUT-503 course project's website (Alt + h)"><img src=/uploads/zap.svg alt="Welcome to my CMPUT-503 course project's website" width=35 height=35></img><h3 style=margin-bottom:0><strong><em>CMPUT-503 Robotics Project</em></strong></h3></a></li><li class=toggle-container><input type=checkbox id=switch>
<label for=switch><i id=darkIcon data-feather=moon></i>
<i id=lightIcon data-feather=sun></i></label></li></ul><ul class=desktop-navigation><li><a href=/blog title=Blog><i data-feather=pen-tool></i>
<span>Blog</span></a></li><li><a href=/search title="Search (Alt + /)" accesskey=/><i data-feather=search></i>
<span>Search</span></a></li><li><a href=https://github.com/jihoonog/CMPUT-503 title=Github><i data-feather=github></i>
<span></span></a></li></ul><ul class=mobile-navigation><li><button id=menuOpen onclick=openMobile() aria-label="Menu closed">
<i data-feather=menu></i></button>
<button id=menuClose onclick=openMobile() aria-label="Menu opened">
<i data-feather=x></i></button></li></ul></nav><aside class=sidebar id=mobileNav style=display:none><nav><ul><li><a href=/blog title=Blog><span><i data-feather=pen-tool></i>
Blog</span></a></li><li><a href=/search title="Search (Alt + /)" accesskey=/><span><i data-feather=search></i>
Search</span></a></li><li><a href=https://github.com/jihoonog/CMPUT-503 title=Github><span><i data-feather=github></i></span></a></li></ul></nav></aside><main><div class=container><hgroup><h1>Exercise 3</h1><time datetime="2023-03-03 00:00:00 +0000 UTC">March 3, 2023</time>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Jihoon Og<p>Third Lab Assignment</p></hgroup><a href=https://quackquack.forkprocess.com/tags/lane-following/><kbd>lane following</kbd></a>
<a href=https://quackquack.forkprocess.com/tags/programming/><kbd>programming</kbd></a>
<a href=https://quackquack.forkprocess.com/tags/ros/><kbd>ROS</kbd></a>
<a href=https://quackquack.forkprocess.com/tags/pid/><kbd>pid</kbd></a>
<a href=https://quackquack.forkprocess.com/tags/sensor-fusion/><kbd>sensor fusion</kbd></a><div class="grid grid-main"><div><article class=toc><div><details><summary accesskey=c title="(Alt + C)"><span>Table of Contents</span></summary><div><ul><li><a href=#team-members aria-label="Team Members">Team Members</a></li><li><a href=#part-1---computer-vision aria-label="Part 1 - Computer Vision">Part 1 - Computer Vision</a><ul><li><a href=#questions aria-label=Questions>Questions</a></li><li><a href=#video aria-label=Video>Video</a></li></ul></li><li><a href=#part-2---lane-following aria-label="Part 2 - Lane Following">Part 2 - Lane Following</a><ul><li><a href=#implementation aria-label=Implementation>Implementation</a></li><li><a href=#questions-1 aria-label=Questions>Questions</a></li><li><a href=#video-demo aria-label="Video Demo">Video Demo</a></li></ul></li><li><a href=#part-3---localization-using-sensor-fusion aria-label="Part 3 - Localization Using Sensor Fusion">Part 3 - Localization Using Sensor Fusion</a><ul><ul><li><a href=#section-31-32 aria-label="Section 3.1-3.2">Section 3.1-3.2</a><ul><li><a href=#questions-2 aria-label=Questions>Questions</a></li></ul></li><li><a href=#section-33 aria-label="Section 3.3">Section 3.3</a></li><li><a href=#section-34 aria-label="Section 3.4">Section 3.4</a><ul><li><a href=#question aria-label=Question>Question</a></li></ul></li><li><a href=#section-35 aria-label="Section 3.5">Section 3.5</a><ul><li><a href=#questions-3 aria-label=Questions>Questions</a></li></ul></li><li><a href=#section-36 aria-label="Section 3.6">Section 3.6</a><ul><li><a href=#questions-4 aria-label=Questions>Questions</a></li></ul></li><li><a href=#37 aria-label=3.7>3.7</a></li></ul></ul></li><li><a href=#repo-link aria-label="Repo Link">Repo Link</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div></article><div class=post><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on twitter" href="https://twitter.com/intent/tweet/?text=Exercise%203&url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f&hashtags=lanefollowing%2cprogramming%2cROS%2cpid%2csensorfusion"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f&title=Exercise%203&summary=Exercise%203&source=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f&title=Exercise%203"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on whatsapp" href="https://api.whatsapp.com/send?text=Exercise%203%20-%20https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on telegram" href="https://telegram.me/share/url?text=Exercise%203&url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div><p>This is the third lab assignment of the course.</p><h1 id=team-members>Team Members<a class=anchor aria-hidden=true href=#team-members>#</a></h1><ul><li>Jihoon Og</li><li>Qianxi Li</li></ul><h1 id=part-1---computer-vision>Part 1 - Computer Vision<a class=anchor aria-hidden=true href=#part-1---computer-vision>#</a></h1><p>Part 1 was primarily done by my lab partner Qianxi Li, the report is mostly from his website <a href=https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3 target=_blank rel=noopener>here</a></p><p>For this exercise, we were required to interact with the transformation between different frames in the Duckietown environment. We first needed to recognize the different AprilTags using the built-in computer vision library. For section 1 of exercise 3, we basically needed to undo the distortion caused by the camera, then use the AprilTags detector library to get the tag&rsquo;s id, location, rotation pose, and corner locations within the camera frame. We then needed to label the edge of the tag and give a type to it. Depending on the type of the AprilTag, we needed to change the color of the LED emitter based on the table below:</p><ul><li>Red: Stop Sign</li><li>Blue: T-Intersection</li><li>Green: U of A Tag</li><li>White: No Detections</li></ul><p>In certain situations the camera may capture multiple tags within it&rsquo;s field of view. However, we only want to detect the closest one. So we only keep the one with the smallest z-value (closest to our camera) and the clamp the maximum z-value to be accepted to 0.5 meters. In addition, the margin of our detection should be larger than 10 to get classified as an AprilTag with a strong confidence.</p><p>Because the lens of the camera distort our image, the lines within the image are not straight. Therefore, when people deal with 3-D tasks from photo image input, an undistortion stage is required.</p><h2 id=questions>Questions<a class=anchor aria-hidden=true href=#questions>#</a></h2><ol><li>What does the april tag library return to you for determining its position?</li></ol><p>For each tag it detects, it returns the x, y coordinates for the tag&rsquo;s center and its corners. It uses the image coordinate system meaning the origin starts in the top left corner and increases as you go right and down. It can also estimate the pose of the tag with respect to the camera in (x,y,z).</p><ol start=2><li>Which directions do the X, Y, Z values of your detection increase / decrease?</li></ol><p>The z-axis points from the camera&rsquo;s center out from the camera&rsquo;s lens.
The x-axis is to the right in the image taken by the camera.
The y-axis goes down from the top frame of the camera.
So moving away from the camera will increase the z value, moving to the right of the camera will increase the x value, and moving downward of the camera will crease the y value.</p><ol start=3><li>What frame orientation does the april tag use?</li></ol><p>The tag&rsquo;s coordinate frame is centered at the center of the tag, with the x-axis to the right, y-axis pointing down, and the z-axis pointing into the tag.</p><p><a href=https://github.com/AprilRobotics/apriltag/wiki/AprilTag-User-Guide target=_blank rel=noopener>https://github.com/AprilRobotics/apriltag/wiki/AprilTag-User-Guide</a></p><ol start=4><li>Why are detections from far away prone to error?</li></ol><p>Tags that are further away will make it harder for the camera to distinguish the smaller &ldquo;pixels&rdquo; that identifies the AprilTags. Moreover, the focal length is set for a particular distance from the camera and objects that are farther away from the focal length will be out-of-focus thus making it harder for the detection algorithm to find and identify the tag.</p><ol start=5><li>Why may you want to limit the rate of detections?</li></ol><p>Detecting the AprilTags too frequently will add unnecessary overhead to the system adding additional latency that could negatively affect other systems that are critical.</p><h2 id=video>Video<a class=anchor aria-hidden=true href=#video>#</a></h2><p>The video below shows how the Duckiebot detects and labels the AprilTag using a bounding box and a text label. A tag with a type of intersection is detected by the camera.</p><iframe src=https://drive.google.com/file/d/1DAXbdUZXFY1npf_VjEOVV97hlnL_UAvL/preview width=640 height=480 allow=autoplay></iframe><h1 id=part-2---lane-following>Part 2 - Lane Following<a class=anchor aria-hidden=true href=#part-2---lane-following>#</a></h1><p>This part was primarily done by me.</p><h2 id=implementation>Implementation<a class=anchor aria-hidden=true href=#implementation>#</a></h2><p>In order to ease development of the lane following functionality we&rsquo;ve decided to reuse the lane following pipeline from the <code>dt-core</code> module. Below is a diagram from the Duckietown documentation showing an overview of the lane following pipeline.</p><p><img src=/uploads/data-from-img-image_pipeline_overview-77bbe28a.png alt="lane following pipeline"></p><p>For this lab exercise we&rsquo;ve implemented our own lane controller node that take in <code>lane pose</code> generated by the lane filter node from the Duckiebot&rsquo;s built-in lane following pipeline. The main difference between their implementation and mine is that We use all three PID parameters (Proportional, Integral, and Derivative) for both the lateral and angular error while they use only proportional and integral terms for the lateral and angular error. The main rational is to further improve performance by using a <code>D</code> term to decrease overshoot as well settling time.</p><h2 id=questions-1>Questions<a class=anchor aria-hidden=true href=#questions-1>#</a></h2><ol><li>What is the error for your PID controller?</li></ol><p>We used two errors, one is the lateral error, and the other is the angular error from the centerline. While we could just use the lateral error for correction, we&rsquo;ve decided to use bot the lateral and angular error as they are provided by the lane filter node. Moreover, having the additional angular error will help with the lane following as the Duckiebot could be in the center of a lane but at an off angle where moving forward will cause it to drift out of the lane.</p><ol start=2><li>If your proportional controller did not work well alone, what could have caused this?</li></ol><p>Using proportional control led to decent on-center performance on a straight lane tile, however it led to poor on-center performance during a turn. This is
because the proportional term only handles errors happening at present time. On a straight lane if the robot was already aligned with the center of the lane very little correction is needed so long as the trim is set correctly. However, during a turn the center of the lane is constantly moving thus requiring the Duckiebot to make constant corrections. If the proportional term is too small then it can&rsquo;t make a large enough correction to stay within the lane. However, if the term is too large then it will over-correct and create oscillation during the straights. Moreover, a happy medium doesn&rsquo;t really exist that works well for both turns and straights. Therefore, we needed to give more feedback.</p><ol start=3><li>Does the <code>D</code> term help your controller logic? Why or why not?</li></ol><p>Adding a <code>D</code> term helped a lot with the overshoot from having too high of a <code>P</code> term. Because the <code>D</code> term tries to look at future trends in error it provides a dampening component if the <code>P</code> and <code>I</code> components leads to an overshoot.</p><ol start=4><li>(Optional) Why or why not was the <code>I</code> term useful for your robot?</li></ol><p>The <code>I</code> term was useful at the turning tiles as there was a constant change in direction as well as the location of the center line with respect to the Duckiebot&rsquo;s lateral axis. As the error grows over time the <code>I</code> component will add more and more corrective effort in order to bring the Duckiebot back to the center of the lane.</p><h2 id=video-demo>Video Demo<a class=anchor aria-hidden=true href=#video-demo>#</a></h2><p>The video below shows the robot performing some basic lane following driving on the right lane:</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/lgFcwYm_tRE style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>The video below shows the robot performing some basic lane following driving on the left lane:</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/MQJUCQqfxbg style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h1 id=part-3---localization-using-sensor-fusion>Part 3 - Localization Using Sensor Fusion<a class=anchor aria-hidden=true href=#part-3---localization-using-sensor-fusion>#</a></h1><p>Part 3 was primarily done by my lab partner Qianxi Li, the report is mostly from his website [here](<a href=https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3 target=_blank rel=noopener>https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3</a></p><p>The top-level goal for this part is to utilize:</p><ol><li>The known locations of all AprilTags in the world frame (from part 1)</li><li>Use the node we wrote in part 1 to detect the presence of April tags in the image to get a better lane following in a world shown below.</li></ol><p><img src="https://lh4.googleusercontent.com/36C5gYovAqNRHZWxK6JLJIC2jyGWC86t_VuJaVaIFslpzLUIPpIhG-tx5SFRfCselZzEKxFOmeNwePCcYBqFtkfXUS7tY-4FD1ztzWoU6m2IS8oBO1S-q7K1xCUe_F243w=w1280" alt=figure></p><p>As we are using the lane following system from part 2, if we detect an AprilTag the camera, we then can figure out where the robot is in the world frame and update its pose to the true location, otherwise, we continue to use odometry which may be inaccurate.</p><p>The image above is the Duckietown environment our Duckiebots are interacting with, in the next several sections, our Duckiebots will travel along the blue lane, and we will examine the odometry and how the frames of robot interact with each other.</p><h3 id=section-31-32>Section 3.1-3.2<a class=anchor aria-hidden=true href=#section-31-32>#</a></h3><p>In these two sections, the two primary things we needed to get familiar with were:</p><ol><li>Using Rviz to visualize different frames and transformations in different forms</li><li>Given some fixed landmarks in the world, compare the path the robot traveled measured in world frame with the measurements produced by odometry.</li></ol><p>We need to use the method <code>static_transform_publisher</code> of TF2 to broadcast the locations and orientations of 10 different Apriltags in the world frame. Rviz can then be used to visualize all of their locations in the world frame.</p><h4 id=questions-2>Questions<a class=anchor aria-hidden=true href=#questions-2>#</a></h4><ol><li>Where did your odometry seem to drift the most? Why would that be?</li></ol><p>It drifted to the right more frequently. One reason could be that it&rsquo;s hard to make a perfect 90-degree turn by only looking at the camera image, usually when it makes the turn, you feel that it’s a little too over, so you adjust the yaw to the opposite angle a little, which confuse the odometry. The lane we are following has 4 right turns, so it’s easy to get errors at each turn and drift to the right.</p><ol start=2><li>Did adding the landmarks make it easier to understand where and when the odometry drifted?</li></ol><p>Yes, adding the landmarks is very easy to see the current location of our Duckiebot and get to know where and when it is drifted.</p><h3 id=section-33>Section 3.3<a class=anchor aria-hidden=true href=#section-33>#</a></h3><p>In this section, we need to obtain a transform tree graph, which is a tree structure with nodes being different frames and edges being transformations.</p><p>To generate the graph, we did the following procedure:</p><ol><li>Go to dashboard</li><li>Open portainer</li><li>Open a console of the ROS container</li><li>Install ros-noetic-tf2-tools</li><li>Run <code>rosrun tf2_tools view_frames.py</code> to get the PDF file</li><li>Copy it to /data and download it.</li></ol><p>The transform tree graph looks like this, you can see the root is <code>vehicle_name/footprint</code></p><p><img src="https://lh3.googleusercontent.com/sRYc-i1qH_UMjq93NMKehpr7h8TmQzpyaqk3HkwYgjEXjktIvtU_Q0QFzqcTG2nF_1xgQ_FaTNFw5g2Zu9SDhHTp5A5e4AW2qBTUsXfK8XlgdZ9cBfexfiBInFLO8pbm3w=w1280" alt="transform graph tree"></p><h3 id=section-34>Section 3.4<a class=anchor aria-hidden=true href=#section-34>#</a></h3><p>For this section, we can visualize all the frames in the Duckiebot using RViz, and figure out which joint is responsible for rotating the wheels.</p><h4 id=question>Question<a class=anchor aria-hidden=true href=#question>#</a></h4><ol><li>What&rsquo;s the type of joint that moves when we move the wheels?</li></ol><p>The joint is continuous. Since we need to <strong>rotate</strong> the wheel to move it and a continuous joint can rotate around the axis and has no upper and lower limits.</p><h3 id=section-35>Section 3.5<a class=anchor aria-hidden=true href=#section-35>#</a></h3><p>In this section, we want to see both the frames on a Duckiebot, and it should also change the location and orientation in the world frame if we move it. To do this, we let the parent frame be the odometry frame and the child frame be the <code>/footprint frame</code>, since the odometry frame is the child to the world frame, the root is now the world frame. The image below shows the current transform graph:</p><p><img src="https://lh3.googleusercontent.com/brH7s-9P2wNgjta15R-pXLdNRBceB501rVq83dPG3pPgq-XbUK00XqlfmIkBL07fUWNA5IoOz-6ULr6rONPHjR6pNiAzYcXBZA_kYvYoRxH3iWQi8q1UxnWCQdTKu9pYhg=w1280" alt="transform graph tree 2"></p><p>The reason why no frame is moving is that we are now using the baselink frame (<code>/footprint</code>) of the Duckiebot, whenever you move the robot, the frame will also be there and the relative location of different frames on a Duckiebot will be the same.</p><h4 id=questions-3>Questions<a class=anchor aria-hidden=true href=#questions-3>#</a></h4><ol><li>What should the translation and rotation be from the odometry child to robot parent frame? In what situation would you have to use something different?</li></ol><p>The translation and rotation should all be zero. We should consider changing the values when the robot root frame is not the same as the odometry frame.</p><ol start=2><li>After creating this link generate a new transform tree graph. What is the new root/parent frame for your environment?</li></ol><p>The new root is <code>/world</code>, the world frame.</p><ol start=3><li>Can a frame have two parents? What is your reasoning for this?</li></ol><p>No, a frame cannot have two parents. The transform tree graph means, for every two nodes in the same tree, it is possible to have a transformation between them, but if you have two parents of the same frame, you cannot have a transformation that transforms one to another. And TF expects a tree structure and cannot deal with the case that has multiple parents.</p><ol start=4><li>Can an environment have more than one parent/root frame?</li></ol><p>Yes, an environment can have more than one parent frame, just like before 3.5, we have a parent world frame with child AprilTag static frames and the odometry frame. And another parent footprint frame with a couple of other frames on the robot. And you cannot have one frame in a tree that transforms into another frame in another tree.</p><h3 id=section-36>Section 3.6<a class=anchor aria-hidden=true href=#section-36>#</a></h3><p>The video below shows how the robot moves in the world frame, where are the estimated location of the AprilTags, and the ground truth locations visualized by RViz. The delay is high since at the time we record this video, there were many people working in the lab causing high network contention. The estimated location of the AprilTags is obtained in the following way:</p><ol><li>We have the rotation and location information in the camera frame from the raw image</li><li>Then we transformed that from the camera frame to the estimated location relative to the camera frame. You can also see all the frames on the robot are also moving as the robot moves, that&rsquo;s because in section 3.5 we attached the odometry frame with the robot root frame.</li></ol><iframe src=https://drive.google.com/file/d/1qPbVoIe3fzPE6ydsNJZNGGuvadNQeenC/preview width=640 height=480 allow=autoplay></iframe><h4 id=questions-4>Questions<a class=anchor aria-hidden=true href=#questions-4>#</a></h4><ol><li>How far off are your detections from the static ground truth?</li></ol><p>If an AprilTag is detected in the image, the estimated AprilTag&rsquo;s frame will show up in RViz immediately. One issue with our design is that after one detection disappeared from the image, the last broadcasted frame will still be there and it will also rotate and move relative to the camera until we have a new April tag detected.</p><p>The error between the ground truth and the estimation is within 20 centimeters, if there&rsquo;s a tag detected in the current image.</p><ol start=2><li>What are two factors that could cause this error?</li></ol><ul><li><p>The resolution of the image can be low, since we can control the resolution of the image we pass to the detector, the higher it is, the more computation we will do, the more accurate location of the tag we can obtain (corner, edge&mldr;)</p></li><li><p>The light and the angle at that the camera observes the tag also matter. The dimmer the room is, the smaller the angle is, the harder for the camera to estimate its location and other information.</p></li></ul><h3 id=37>3.7<a class=anchor aria-hidden=true href=#37>#</a></h3><ol><li>Is this a perfect system?</li><li>What are the causes for some of the errors?</li><li>What other appraoches could you use to improve localization?</li></ol><h1 id=repo-link>Repo Link<a class=anchor aria-hidden=true href=#repo-link>#</a></h1><p><a href>Exercise 3 repository link (TODO)</a></p><h1 id=references>References<a class=anchor aria-hidden=true href=#references>#</a></h1><p>This is a list of references that I used to do this exercise.</p><ol><li>Lane Controller Node: <a href=https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/src/lane_controller_node.py target=_blank rel=noopener>https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/src/lane_controller_node.py</a></li><li>Lane Controller: <a href=https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/include/lane_controller/controller.py target=_blank rel=noopener>https://github.com/duckietown/dt-core/blob/6d8e99a5849737f86cab72b04fd2b449528226be/packages/lane_control/include/lane_controller/controller.py</a></li><li>PID Controller: <a href=https://en.wikipedia.org/wiki/PID_controller target=_blank rel=noopener>https://en.wikipedia.org/wiki/PID_controller</a></li><li>PID controller code: <a href=https://github.com/jellevos/simple-ros-pid/blob/master/simple_pid/PID.py target=_blank rel=noopener>https://github.com/jellevos/simple-ros-pid/blob/master/simple_pid/PID.py</a></li><li>Qianxi Li&rsquo;s website: <a href=https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3 target=_blank rel=noopener>https://sites.google.com/ualberta.ca/qianxi-duckie/exercises/exercise-3</a></li><li>URDF Joints: <a href=http://wiki.ros.org/urdf/XML/joint target=_blank rel=noopener>http://wiki.ros.org/urdf/XML/joint</a></li><li>URDF parameters for the Duckiebot: <a href=https://github.com/duckietown/dt-duckiebot-interface/blob/56a299aa5739e7f03a6b96d3b8dac3a8beca532c/packages/duckiebot_interface/urdf/duckiebot.urdf.xacro target=_blank rel=noopener>https://github.com/duckietown/dt-duckiebot-interface/blob/56a299aa5739e7f03a6b96d3b8dac3a8beca532c/packages/duckiebot_interface/urdf/duckiebot.urdf.xacro</a></li><li>TF2: <a href=http://wiki.ros.org/tf2/Tutorials/Introduction%20to%20tf2 target=_blank rel=noopener>http://wiki.ros.org/tf2/Tutorials/Introduction%20to%20tf2</a></li><li>TF2 Static Broadcaster: <a href=http://wiki.ros.org/tf2/Tutorials/Writing%20a%20tf2%20static%20broadcaster%20%28Python%29 target=_blank rel=noopener>http://wiki.ros.org/tf2/Tutorials/Writing%20a%20tf2%20static%20broadcaster%20%28Python%29</a></li><li>TF transformations: <a href=http://wiki.ros.org/tf/Overview/Transformations target=_blank rel=noopener>http://wiki.ros.org/tf/Overview/Transformations</a></li><li>Sensor Fusion: <a href=https://docs.duckietown.org/daffy/duckietown-classical-robotics/out/exercise_sensor_fusion.html#fig:rviz-final-tf-tree target=_blank rel=noopener>https://docs.duckietown.org/daffy/duckietown-classical-robotics/out/exercise_sensor_fusion.html#fig:rviz-final-tf-tree</a></li><li>AprilTag with Python: <a href=https://pyimagesearch.com/2020/11/02/apriltag-with-python/ target=_blank rel=noopener>https://pyimagesearch.com/2020/11/02/apriltag-with-python/</a></li></ol></div><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on twitter" href="https://twitter.com/intent/tweet/?text=Exercise%203&url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f&hashtags=lanefollowing%2cprogramming%2cROS%2cpid%2csensorfusion"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f&title=Exercise%203&summary=Exercise%203&source=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f&title=Exercise%203"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on whatsapp" href="https://api.whatsapp.com/send?text=Exercise%203%20-%20https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Exercise 3 on telegram" href="https://telegram.me/share/url?text=Exercise%203&url=https%3a%2f%2fquackquack.forkprocess.com%2fblog%2fexercise-3%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></div></div></div></main><footer class=text-center><span>A template by <a href=https://www.heksagon.net title="Your most friendly developer">Heksagon</a> &copy; 2023 Piko</span><div><a href=https://quackquack.forkprocess.com/legal/privacy>Privacy Policy</a>
<span>&</span>
<a href=https://quackquack.forkprocess.com/legal/terms-and-conditions>Terms and Conditions</a></div></footer><article class=hidden id=cookie-banner><div><span>We use cookies to improve your experience on our site and to show you relevant advertising.</span></div><footer><a role=button id=consent-cookies>Close</a>
<a role=button class=secondary href=/legal/privacy/#cookies-and-web-beacons>Cookies Policy</a></footer></article><script async src=https://quackquack.forkprocess.com/js/cookie.min.js layout=container></script>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script><script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('kbd');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script><script src=https://quackquack.forkprocess.com/plugins/js/feather.min.js layout=container></script>
<script src=https://quackquack.forkprocess.com/js/script.min.js layout=container></script></body></html>